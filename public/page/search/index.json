[{"content":"Introduction It is common that one would have multiple Github accounts, such as one for personal use and one for work. Managing Github repositories requires a developer to set up SSH keys on his/her computer. However, this becomes non-trivial when one has to work with multiple accounts representing different identities. This blog post describes how one can easily manage multiple Github repositories from different accounts on the same computer.\nScenario Let\u0026rsquo;s assume that I have a two Github accounts, one is albert-personal and one is albert-work. I use the former to work on my personal projects, and use the latter when working on repositories from my work. Let\u0026rsquo;s also assume that we have two repositories, one is personal-project and another is work-project, which are under the two accounts mentioned above respectively.\nAlso when accessing these two accounts, I need to use different SSH keys. Let\u0026rsquo;s say I have the following keys in my .ssh folder:\nalbert-personal # private key for personal account albert-personal.pub # public key for personal account albert-work # private key for work account albert-work.pub # private key for work account Our goal is to set up the repositories to use the corresponding SSH key when pushing and pulling from Github.\nSolution We want to set up the local repository to use a certain SSH key. To do so, we first create a new config file under the .ssh folder.\nFirstly, we create a file ~/.ssh/config-personal with the following content:\nHost github.com HostName github.com Port 22 User git IdentifyFile ~/.ssh/albert-personal and also a file ~/.ssh/config-work with the following content\nHost github.com HostName github.com Port 22 User git IdentifyFile ~/.ssh/albert-work These configuration files basically tells the ssh program to use a certain SSH key when accessing a certain domain (github.com in this case).\nNext we need to configure each local repository to use the corresponding SSH configuration file when pushing or pulling from Github. Every Git repository has a .git folder, in which there is a file named config that stores some configurations of the repository. It looks something like this:\n[core] repositoryformatversion = 0 filemode = true bare = false logallrefupdates = true ignorecase = true precomposeunicode = true [remote \u0026#34;origin\u0026#34;] url = git@github.com:username/repo-name.git fetch = +refs/heads/*:refs/remotes/origin/* [branch \u0026#34;main\u0026#34;] remote = origin merge = refs/heads/main We can add a new line into this config file to ask git to use a certain SSH configuration file. This is done by adding the following line under the [core] section in personal-project/.git/config:\nsshCommand = ssh -F ~/.ssh/config-personal and in work-project/.git/config:\nsshCommand = ssh -F ~/.ssh/config-work Finally, make sure that you have added the SSH public keys to the corresponding Github account.\n","date":"2023-08-27T00:00:00Z","permalink":"https://albertauyeung.github.io/2023/08/27/multiple-github-account.html","title":"💻 Managing multiple Github accounts on the same computer"},{"content":"\nLike most people, I first knew about Andy Weir through the movie Martian. It was a great sci-fi movie with the science presented accurately. Therefore, when I knew that Andy Weir\u0026rsquo;s latest science fiction, Project Hail Mary, was released, I immediately bought the book and started reading.\nProject Hail Mary tells the story of Ryland Grace, a high school scienc teacher, who was involved in a mission to save the Solar System. It turns out to be another great science fiction, which in my opinion won\u0026rsquo;t disappoint anyone who is attracted to it because of Andy Weir\u0026rsquo;s previous fictions. It definitely is the best book I read in 2021. I was later delighted to see that Bill Gates featured Project Hail Mary as one of the five books he loved reading this year, and the book was also the winner of in the sci-fi category in the 2021 Goodreads Choice Award.\nIn many aspect, Project Hail Mary is similar to Martian. Both protagonists are scientists who were thrown into desperate situations in which they have to fight for their own survival with their scientific knowledge. However, in Project Hail Mary, Ryland\u0026rsquo;s purpose is not only to survive but also to make sure that the mission to save the solar system and the human race. This adds more complexity into the story, as well as the decision making processes that Ryland has to gone through.\nAlthough I enjoy different types of science fiction, I always find it more engaging when the aurthor attempts to present sciecne as accurate as possible and to explain things that appear in the story with existing scientific knowledge. That is why I enjoy reading Project Hail Mary very much. I highly recommend the book to anyone who is looking for something fun and inspiring to read over a weekend.\n","date":"2021-12-20T00:00:00Z","permalink":"https://albertauyeung.github.io/2021/12/20/project-hail-mary.html","title":"🛸 Project Hail Mary"},{"content":"最近在看 Daniel Kahneman 的新書 Noise（中譯《雜訊》）。Daniel Kahneman 是2002年諾貝爾經濟學奬得主，他的前作 Thinking Fast and Slow（中譯《快思慢想》）基於他跟拍檔 Amos Tversky 多年來的研究，介紹人類的兩大思考模式，引起很大迴響。如果没有看過該書，可以先看看 Kahneman 2011年在 Google 的演講。這次的新書，則主要介紹及說明決策過種由於各種原因出現的雜訊，導致人們在面對相同的問題時也可以給出完全不一樣的判斷。他指出，這種在決策中現的雜訊隨處可見，書中的例子更是令人大開眼界。\n在美國有人曾經研究過法官的判刑決定，不同的法官對某一案件的判刑可以是一年到十五年，有些研究更指出法官的判決跟會受一些看來毫不相關的因素影響。例如，如果本地的足球隊在週末輸了一場比賽，法官們在接下來的星期一所作出的判決會比較重。在法國，如果判決當天是辯護人的生日，法官作出的判決會較輕。書中另一個例子發生在 Kahneman 自己接觸過的一家保險公司。他發現保險公司不同員工在估算同一保單的價格或賠償金額時所作的決定並不一致，差額更可以超過 50%，而公司的管理層卻一直認為不同員工所作的決定相差不會超過 10%。\n更有趣的一項研究見於一位來自西班牙的行為心理學教授 Uri Simonsohn 發表的一篇論文，題為 \u0026ldquo;Clouds Make Nerds Look Good\u0026quot;。他發現天氣對大學入學申請的審批居然有明顯的影響： 天氣不好的時候，負責審批申請的人員會更看重學生的學術成績，而在天氣晴朗的日子，在其他非學術方面表現突出的學生會比較有利！\nKahneman 在書中指出，決策的錯誤來自兩個因素，一為「偏見」(bias)，二為「雜訊」(noise)（更為正確的名稱應該是 variance，即判斷偏離平均值的程度）。一般情況下，人們更容易察覺到偏見的存在，而忽略雜訊，但修正偏見跟減低雜訊，對避免作出錯誤判斷的貢獻是一樣的。故此他認為我們不應對決策過程中出現的雜訊視而不見。\n作為一個在工作中經常應用到各種機器學習技術的軟件工程師，這些概念一點都不陌生。其實，bias 和 variance 正是機器學習中非常重要的概念，主要用於分析一個模型是否準確描述數據之間的關係。\n一個模型 bias 比較大時，說明它過份簡單，未能完全反映現實世界中事物之間的複雜關係。例如，當要預測一個人是否有心臟病時，如果模型只認為年齡越大，則有心臟病的機率越大，很明顯這個模型不會十分準確。換句話說，這個模型的「偏見」十分嚴重（過份簡單），因為它只參考一個人的其中一個特徴來作出判斷，而且只考慮此特徵跟結果的線性關係。 另一方面，一個模型的 variance 比較大時，說明它過份複雜，在學習數據之間的關係時，會把一些本來跟結果没有關係、只是偶爾出現在個別情況的特徵也納入考慮，導致模型在將來的應用中未能作出準確的判斷（因為它很容易被數據中的雜訊誤導），也就是在訓練模型時經常會提及的所謂 overfitting（過度擬合）。 一般人可能認為在作出各種專業判斷的時候，專業知識是最重要的因素，但 Kahneman 在這本書裡面明確指出決策過程中的雜訊跟專業知識是否全面没有必然關係。要避免錯誤的判斷，除了要排除「偏見」/擁有全面的知識外，更要關注作出判斷時的所出現的雜訊。\n","date":"2021-11-08T00:00:00Z","permalink":"https://albertauyeung.github.io/2021/11/08/noise-daniel-kahneman.html","title":"🔊 Noise 雜訊"},{"content":"It is common that the different projects you are working on depend on different versions of Python. That is why pyenv becomes very handy for Python developers, as it lets you switch between different Python versions easily. With pyenv-virtualenv it can also be used together with virtualenv to create isolated development environments for different projects with different dependencies.\nFor example, if some of the projects you are working on requires Tensorflow 1.15, while your system\u0026rsquo;s Python is of version 3.8, you must find some ways to install Python 3.7 in order to work on your project, as Tensorflow 1.15 can only be run in Python 3.5 to Python 3.7.\nThis article aims at giving a quick introduction to pyenv and pyenv-virtualenv, as well as describing how one can easily create new kernels of virtual environments in Jupyter.\nInstalling and Using pyenv pyenv works on macOS and Linux, but not Windows (except inside the Windows Subsystem for Linux). Windows users might want to check out pyenv-win for further information.\nOn macOS, it can be installed using Homebrew:\n$ brew update $ brew install pyenv On both macOS and Linux, it can also be installed by checking out the latest version of pyenv. For details of installing pyenv this way, refer to the offical installation guidelines here: https://github.com/pyenv/pyenv#installation.\nAfter installation, add the following line to your .bashrc (or .zshrc) file:\neval \u0026#34;$(pyenv init -)\u0026#34; Once you have pyenv installed, you can do a few things like below:\nInstalling a Python version\n# List all available Python versions $ pyenv install --list # Install a specific Python version (3.7.8) $ pyenv install 3.7.8 # List Python version installed $ pyenv versions * system (set by /Users/....) 3.7.8 Setting a local Python version\n# Set the Python version for the current directory $ pyenv local 3.7.8 # Now by default you will be using Python 3.7.8 $ python Python 3.7.8 (default, Aug 17 2020, 11:05:21) \u0026gt;\u0026gt;\u0026gt; # Unset it and change back to system default $ pyenv local --unset Setting a global Python version\n# Install a new version and set it as system default $ pyenv install 2.7.6 $ pyenv global 2.7.6 # Now you have 2.7.6 as the default Python version $ python Python 2.7.6 (default, Aug 17 2020, 11:08:23) \u0026gt;\u0026gt;\u0026gt; Using virtualenv with pyenv pyenv by itself only allows you to switch between different Python versions. To create an isolated environment with a set of dependencies, we will need virtualenv too. You can follow the steps below to set up your computer to use pyenv and virtualenv together.\nFirstly, we need ot install virtualenv:\n$ pip3 install virtualenv $ pip3 install virtualenvwrapper Next, we need to install pyenv-virtualenv. This can be done on macOS by using brew as follows (or follow the instructions on this page if you are not using macOS):\n$ brew install pyenv-virtualenv Finally, add the following line to your .bashrc or .zshrc file:\neval \u0026#34;$(pyenv virtualenv-init -)\u0026#34; Once you are done with the steps above, you can create new virtual environments as follows:\n# Install a new Python version $ pyenv install 3.7.4 # Create a new virtualenv named myenv with Python 3.7.4 $ pyenv virtualenv 3.7.4 tf1.15 # Go to the project directory, and set its local environment $ cd ~/repo/my-project $ pyenv local tf1.15 # Install dependencies as needed $ pip3 install tensorflow==1.15 Adding Kernels to Jupyter It is also common that we use Jupyter for quick prototyping and testing. It would be convenient if we can invoke different virtual environments in Jupyter to test our source codes. In fact, it is very easy to create new kernels of different virtual environments in Jupyter.\nFirstly, you have to check the paths of your Juypyter installation. (Note that it does not matter which environment you are using to run your Jupyter notebook or Jupyter lab.) You can check the paths using the following command:\n$ jupyter --paths On my computer, it is something like below. What we need to note here is the data path.\nconfig: /Users/albert/.jupyter /usr/local/Cellar/python@3.8/3.8.4/Frameworks/Python.framework/Versions/3.8/etc/jupyter /usr/local/etc/jupyter /etc/jupyter data: /Users/albert/Library/Jupyter /usr/local/Cellar/python@3.8/3.8.4/Frameworks/Python.framework/Versions/3.8/share/jupyter /usr/local/share/jupyter /usr/share/jupyter runtime: /Users/ayeung/Library/Jupyter/runtime Next, we will need to check the path to the Python interpreter of the virtual environment:\n# Activate your virtualenv $ pyenv activate tf1.15 # Check path of the Python interpreter $ pyenv which python /Users/albert/.pyenv/versions/tf1.15/bin/python # Deactivate the virtualenv $ pyenv deactivate Finally, we create a new folder under the kernels directory:\n$ mkdir /User/albert/Library/Jupyter/kernels/tf1.15 and add a new file named kernel.json in that directory with the following content:\n{ \u0026#34;argv\u0026#34;: [ \u0026#34;/User/albert/.pyenv/versions/tf1.15/bin/python\u0026#34;, \u0026#34;-m\u0026#34;, \u0026#34;ipykernel\u0026#34;, \u0026#34;-f\u0026#34;, \u0026#34;{connection_file}\u0026#34; ], \u0026#34;display_name\u0026#34;: \u0026#34;tf1.15\u0026#34;, \u0026#34;language\u0026#34;: \u0026#34;python\u0026#34; } Once this is done, you will be able to use the kernel in Jupyter.\n","date":"2020-08-17T00:00:00Z","permalink":"https://albertauyeung.github.io/2020/08/17/pyenv-jupyter.html","title":"🪐 pyenv, virtualenv and using them with Jupyter"},{"content":"To use a pre-trained BERT model, we need to convert the input data into an appropriate format so that each sentence can be sent to the pre-trained model to obtain the corresponding embedding. This article introduces how this can be done using modules and functions available in Hugging Face\u0026rsquo;s transformers package (https://huggingface.co/transformers/index.html).\nInput Representation in BERT Let\u0026rsquo;s first try to understand how an input sentence should be represented in BERT. BERT embeddings are trained with two training tasks:\nClassification Task: to determine which category the input sentence should fall into Next Sentence Prediction Task: to determine if the second sentence naturally follows the first sentence. The [CLS] and [SEP] Tokens For the classification task, a single vector representing the whole input sentence is needed to be fed to a classifier. In BERT, the decision is that the hidden state of the first token is taken to represent the whole sentence. To achieve this, an additional token has to be added manually to the input sentence. In the original implementation, the token [CLS] is chosen for this purpose.\nIn the \u0026ldquo;next sentence prediction\u0026rdquo; task, we need a way to inform the model where does the first sentence end, and where does the second sentence begin. Hence, another artificial token, [SEP], is introduced. If we are trying to train a classifier, each input sample will contain only one sentence (or a single text input). In that case, the [SEP] token will be added to the end of the input text.\nIn summary, to preprocess the input text data, the first thing we will have to do is to add the [CLS] token at the beginning, and the [SEP] token at the end of each input text.\nPadding Token [PAD] The BERT model receives a fixed length of sentence as input. Usually the maximum length of a sentence depends on the data we are working on. For sentences that are shorter than this maximum length, we will have to add paddings (empty tokens) to the sentences to make up the length. In the original implementation, the token [PAD] is used to represent paddings to the sentence.\nConverting Tokens to IDs When the BERT model was trained, each token was given a unique ID. Hence, when we want to use a pre-trained BERT model, we will first need to convert each token in the input sentence into its corresponding unique IDs.\nThere is an important point to note when we use a pre-trained model. Since the model is pre-trained on a certain corpus, the vocabulary was also fixed. In other words, when we apply a pre-trained model to some other data, it is possible that some tokens in the new data might not appear in the fixed vocabulary of the pre-trained model. This is commonly known as the out-of-vocabulary (OOV) problem.\nFor tokens not appearing in the original vocabulary, it is designed that they should be replaced with a special token [UNK], which stands for unknown token.\nHowever, converting all unseen tokens into [UNK] will take away a lot of information from the input data. Hence, BERT makes use of a WordPiece algorithm that breaks a word into several subwords, such that commonly seen subwords can also be represented by the model.\nFor example, the word characteristically does not appear in the original vocabulary. Nevertheless, when we use the BERT tokenizer to tokenize a sentence containing this word, we get something as shown below:\n\u0026gt;\u0026gt;\u0026gt; from transformers import BertTokenizer \u0026gt;\u0026gt;\u0026gt; tz = BertTokenizer.from_pretrained(\u0026#34;bert-base-cased\u0026#34;) \u0026gt;\u0026gt;\u0026gt; tz.convert_tokens_to_ids([\u0026#34;characteristically\u0026#34;]) [100] \u0026gt;\u0026gt;\u0026gt; sent = \u0026#34;He remains characteristically confident and optimistic.\u0026#34; \u0026gt;\u0026gt;\u0026gt; tz.tokenize(sent) [\u0026#39;He\u0026#39;, \u0026#39;remains\u0026#39;, \u0026#39;characteristic\u0026#39;, \u0026#39;##ally\u0026#39;, \u0026#39;confident\u0026#39;, \u0026#39;and\u0026#39;, \u0026#39;optimistic\u0026#39;, \u0026#39;.\u0026#39;] \u0026gt;\u0026gt;\u0026gt; tz.convert_tokens_to_ids(tz.tokenize(sent)) [1124, 2606, 7987, 2716, 9588, 1105, 24876, 119] We can see that the word characteristically will be converted to the ID 100, which is the ID of the token [UNK], if we do not apply the tokenization function of the BERT model.\nThe BERT tokenization function, on the other hand, will first breaks the word into two subwoards, namely characteristic and ##ally, where the first token is a more commonly-seen word (prefix) in a corpus, and the second token is prefixed by two hashes ## to indicate that it is a suffix following some other subwords.\nAfter this tokenization step, all tokens can be converted into their corresponding IDs.\nSummary In summary, an input sentence for a classification task will go through the following steps before being fed into the BERT model.\nTokenization: breaking down of the sentence into tokens Adding the [CLS] token at the beginning of the sentence Adding the [SEP] token at the end of the sentence Padding the sentence with [PAD] tokens so that the total length equals to the maximum length Converting each token into their corresponding IDs in the model An example of preparing a sentence for input to the BERT model is shown below. For simplicity, we assume the maximum length is 10 in the example below (while in the original model it is set to be 512).\n# Original Sentence Let\u0026#39;s learn deep learning! # Tokenized Sentence [\u0026#39;Let\u0026#39;, \u0026#34;\u0026#39;\u0026#34;, \u0026#39;s\u0026#39;, \u0026#39;learn\u0026#39;, \u0026#39;deep\u0026#39;, \u0026#39;learning\u0026#39;, \u0026#39;!\u0026#39;] # Adding [CLS] and [SEP] Tokens [\u0026#39;[CLS]\u0026#39;, \u0026#39;Let\u0026#39;, \u0026#34;\u0026#39;\u0026#34;, \u0026#39;s\u0026#39;, \u0026#39;learn\u0026#39;, \u0026#39;deep\u0026#39;, \u0026#39;learning\u0026#39;, \u0026#39;!\u0026#39;, \u0026#39;[SEP]\u0026#39;] # Padding [\u0026#39;[CLS]\u0026#39;, \u0026#39;Let\u0026#39;, \u0026#34;\u0026#39;\u0026#34;, \u0026#39;s\u0026#39;, \u0026#39;learn\u0026#39;, \u0026#39;deep\u0026#39;, \u0026#39;learning\u0026#39;, \u0026#39;!\u0026#39;, \u0026#39;[SEP]\u0026#39;, \u0026#39;[PAD]\u0026#39;] # Converting to IDs [101, 2421, 112, 188, 3858, 1996, 3776, 106, 102, 0] Tokenization using the transformers Package While there are quite a number of steps to transform an input sentence into the appropriate representation, we can use the functions provided by the transformers package to help us perform the tokenization and transformation easily. In particular, we can use the function encode_plus, which does the following in one go:\nTokenize the input sentence Add the [CLS] and [SEP] tokens. Pad or truncate the sentence to the maximum length allowed Encode the tokens into their corresponding IDs Pad or truncate all sentences to the same length. Create the attention masks which explicitly differentiate real tokens from [PAD] tokens The following codes shows how this can be done.\n# Import tokenizer from transformers package from transformers import BertTokenizer # Load the tokenizer of the \u0026#34;bert-base-cased\u0026#34; pretrained model # See https://huggingface.co/transformers/pretrained_models.html for other models tz = BertTokenizer.from_pretrained(\u0026#34;bert-base-cased\u0026#34;) # The senetence to be encoded sent = \u0026#34;Let\u0026#39;s learn deep learning!\u0026#34; # Encode the sentence encoded = tz.encode_plus( text=sent, # the sentence to be encoded add_special_tokens=True, # Add [CLS] and [SEP] max_length = 64, # maximum length of a sentence pad_to_max_length=True, # Add [PAD]s return_attention_mask = True, # Generate the attention mask return_tensors = \u0026#39;pt\u0026#39;, # ask the function to return PyTorch tensors ) # Get the input IDs and attention mask in tensor format input_ids = encoded[\u0026#39;input_ids\u0026#39;] attn_mask = encoded[\u0026#39;attention_mask\u0026#39;] After executing the codes above, we will have the following content for the input_ids and attn_mask variables:\n\u0026gt;\u0026gt;\u0026gt; input-ids tensor([[ 101, 2421, 112, 188, 3858, 1996, 3776, 106, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]) \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; attn_mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]) The \u0026ldquo;attention mask\u0026rdquo; tells the model which tokens should be attended to and which (the [PAD] tokens) should not (see the documentation for more detail). It will be needed when we feed the input into the BERT model.\nReference Devlin et al. 2018. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding BERT - transformers documentation: https://huggingface.co/transformers/model_doc/bert.html ","date":"2020-06-19T00:00:00Z","permalink":"https://albertauyeung.github.io/2020/06/19/bert-tokenization.html","title":"🤖 Mastering BERT Tokenization and Encoding"},{"content":"What is a Trie? Trie is a very useful data structure. It is commonly used to represent a dictionary for looking up words in a vocabulary.\nFor example, consider the task of implementing a search bar with auto-completion or query suggestion. When the user enters a query, the search bar will automatically suggests common queries starting with the characters input by the user.\nTo implement such a function, we need several things at the backend. The first, obviously, is a list of common queries. Or it can be a list of proper English words for the purpose of auto-completion). Secondly, we will need to have an algorithm to quickly look up words starting with the characters input by the user, and this is where we need to use the trie data structure.\nThe follow example illustrates why a special data structure is necessary to look up words quickly given a prefix:\nThe user inputs the characters en In our dictionary, we have the following words starting with en: english, entertainment Commonly used data structures such as list and dictionary in Python do not allow quick look up of elements stored inside. For example, to see if there is any word having the prefix en in a Python dictionary, we cannot avoid going through each of the keys, resulting in O(n) time, where n is the number of entries in the dictionary Trie is a tree-like data structure made up of nodes. Nodes can be used to store data. Each node may have none, one or more children. When used to store a vocabulary, each node is used to store a character, and consequently each \u0026ldquo;branch\u0026rdquo; of the trie represents a unique word. The following figure shows a trie with five words (was, wax, what, word, work) stored in it.\nHow does a Trie Work? There are two major operations that can be performed on a trie, namely:\nInserting a word into the trie Searching for words using a prefix Both operations involves traversing the trie by starting from the root node. We take a look at each of these operations in more detail.\nInserting Words into the Trie In order to insert a new word into the trie, we need to first check whether any prefix of the word is already in the trie. Therefore, we will start traverse the trie from the root node, and follow the algorithm below:\nSet the current node to be the root node Set the current character as the first character of the input word Check if the current character is a child of the current node If yes, set the current node to be this child node, set the current character to the next character in the input word, and perform this step again If no, it means from this character onwards, we will need to create new nodes and insert them into the trie Below is an illustration of what will happen when we want to add the word won into the trie above.\nFollowing the steps in the algorithm mentioned above, we will arrive at the node o under w, at which point we discover that n is not a child of o, and therefore we create a new node for the character n, and insert it under o.\nSearching in the Trie A common application scenario of the trie data structure is to search for words with a certain prefix, just like the auto-complete or query suggestion function in a search bar.\nWhen given a prefix, we can traverse the trie to check if any word in the trie starts with that prefix. If the prefix is found in the trie, we can then use depth-first traversal to retrieve all the words with that prefix.\nFor example, given the trie illustrated above, which contains the words was, wax, what, word, work and won, let\u0026rsquo;s see what will happen if we want to search for words with the prefix wa:\nStarting from the root node, we are able to find the node w and a From the node a, we can go on to traverse the trie to retrieve all words starting with the prefix wa When we arrive at the node s, we check whether it is the end of a word (yes), and the word was was output Similarity, when we arrive at the node x, the word wax is output Implementing Trie in Python To implement a trie, we can first create a TrieNode class, which can be used to represent a node in the trie. Below is how this class can be implemented.\nclass TrieNode: \u0026#34;\u0026#34;\u0026#34;A node in the trie structure\u0026#34;\u0026#34;\u0026#34; def __init__(self, char): # the character stored in this node self.char = char # whether this can be the end of a word self.is_end = False # a counter indicating how many times a word is inserted # (if this node\u0026#39;s is_end is True) self.counter = 0 # a dictionary of child nodes # keys are characters, values are nodes self.children = {} In this implementation, we want to store also the number of times a word has been inserted into the trie. This allows us to support additional features, such as ranking the words by their popularity.\nGiven the TrieNode class, we can go on to implement the Trie class as follows.\nclass Trie(object): \u0026#34;\u0026#34;\u0026#34;The trie object\u0026#34;\u0026#34;\u0026#34; def __init__(self): \u0026#34;\u0026#34;\u0026#34; The trie has at least the root node. The root node does not store any character \u0026#34;\u0026#34;\u0026#34; self.root = TrieNode(\u0026#34;\u0026#34;) def insert(self, word): \u0026#34;\u0026#34;\u0026#34;Insert a word into the trie\u0026#34;\u0026#34;\u0026#34; node = self.root # Loop through each character in the word # Check if there is no child containing the character, create a new child for the current node for char in word: if char in node.children: node = node.children[char] else: # If a character is not found, # create a new node in the trie new_node = TrieNode(char) node.children[char] = new_node node = new_node # Mark the end of a word node.is_end = True # Increment the counter to indicate that we see this word once more node.counter += 1 def dfs(self, node, prefix): \u0026#34;\u0026#34;\u0026#34;Depth-first traversal of the trie Args: - node: the node to start with - prefix: the current prefix, for tracing a word while traversing the trie \u0026#34;\u0026#34;\u0026#34; if node.is_end: self.output.append((prefix + node.char, node.counter)) for child in node.children.values(): self.dfs(child, prefix + node.char) def query(self, x): \u0026#34;\u0026#34;\u0026#34;Given an input (a prefix), retrieve all words stored in the trie with that prefix, sort the words by the number of times they have been inserted \u0026#34;\u0026#34;\u0026#34; # Use a variable within the class to keep all possible outputs # As there can be more than one word with such prefix self.output = [] node = self.root # Check if the prefix is in the trie for char in x: if char in node.children: node = node.children[char] else: # cannot found the prefix, return empty list return [] # Traverse the trie to get all candidates self.dfs(node, x[:-1]) # Sort the results in reverse order and return return sorted(self.output, key=lambda x: x[1], reverse=True) Below is an example of how this Trie class can be used:\n\u0026gt;\u0026gt;\u0026gt; t = Trie() \u0026gt;\u0026gt;\u0026gt; t.insert(\u0026#34;was\u0026#34;) \u0026gt;\u0026gt;\u0026gt; t.insert(\u0026#34;word\u0026#34;) \u0026gt;\u0026gt;\u0026gt; t.insert(\u0026#34;war\u0026#34;) \u0026gt;\u0026gt;\u0026gt; t.insert(\u0026#34;what\u0026#34;) \u0026gt;\u0026gt;\u0026gt; t.insert(\u0026#34;where\u0026#34;) \u0026gt;\u0026gt;\u0026gt; t.query(\u0026#34;wh\u0026#34;) [(\u0026#39;what\u0026#39;, 1), (\u0026#39;where\u0026#39;, 1)] References Trie, Wikipedia Tries, Brilliant.org Trie (Prefix Tree) - Visualizing the operations on a trie ","date":"2020-06-15T00:00:00Z","image":"https://albertauyeung.github.io/images/query_suggestion.png","permalink":"https://albertauyeung.github.io/2020/06/15/python-trie.html","title":"🌟 Implementing Trie in Python"},{"content":"It has been quite a long time since I have developed a Website. The last one I made was probably the Website that I designed and developed for my brother (http://www.simusic.hk/) many years ago. At that time, the Website was developed using PHP, JQuery and Bootstrap. Since then, I have been mainly working in the areas of machine learning, data mining and natural language processing, and have left frontend development behind.\nFrontend technologies, like many other areas in software development, have changed a lot in the past years. I know about Angular, React and Vue.js, but have never really gone back to learn the basics until now. Recently, I would like to explore the idea of quickly building a Website that shows dynamic content stored in a database. After some exploration, I found that the following combination seems a quick and convenient way to achieve something similar:\nUse Vue.js to build the frontend Web app Use Google Sheets to store the data Publish the Google Spreadsheet online, and retrieve the data in JSON format in the Web app This post describes how the above idea is used to implement a Web app that displays quotations stored in a Google Spreadsheet. The source code can be found on the github repository, and the Webapp can be found published here https://albertauyeung.github.io/quotes-app/. A screenshot is shown below:\nA Web App for Browsing and Searching Quotes In this small project, I developed a Web app for browsing and search quotes that are stored on a Google Spreadsheet.\nFor simplicity, this app does not allow modifying the data in the spreadsheet via the Web UI. Data has to be manipulated on the Google Sheets directly. However, this allows us to skip using the Google Sheets API in this project.\nGoogle Spreadsheet for Data Storage A google spreadsheet has been created to store the data, which can be found here. For the purpose of my project, I have a table with the following columns:\nquote: the quotation author: the person who wrote or said the text source: the source of the quotation, e.g. the name of a book tags: tags separated by commas insert_date: date on which the quotation was inserted into this sheet (press ctrl+; on Windows or Cmd+; on MacOS to quickly insert the date of today in a Google spreadsheet) In order for the spreadsheet\u0026rsquo;s data to be avaible to other apps without the need to perform authentication, we need to first publish it. This can be done by choosing Publish to the Web from the File menu in Google Sheets.\nTo access the data stored in the spreadsheet, we can send a HTTP GET request to the following URL:\nhttps://spreadsheets.google.com/feeds/list/{sheet_id}/1/public/values?alt=json You will have to replace {sheet_id} with the ID of the Google spreadsheet in which data is stored.\nSending a request to the above URL will result in a JSON response in the following format:\n{ \u0026#34;version\u0026#34;: \u0026#34;1.0\u0026#34;, \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34;, \u0026#34;feed\u0026#34;: { \u0026#34;xmlns\u0026#34;: \u0026#34;http://www.w3.org/2005/Atom\u0026#34;, ... \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;$t\u0026#34;: \u0026#34;Sheet1\u0026#34; }, \u0026#34;link\u0026#34;: [...], ... \u0026#34;entry\u0026#34;: [ { \u0026#34;id\u0026#34;: { ... }, ... \u0026#34;gsx$quote\u0026#34;: { \u0026#34;$t\u0026#34;: \u0026#34;Wisest is she who knows she does not know.\u0026#34; }, \u0026#34;gsx$author\u0026#34;: { \u0026#34;$t\u0026#34;: \u0026#34;Jostein Gaarder\u0026#34; }, \u0026#34;gsx$source\u0026#34;: { \u0026#34;$t\u0026#34;: \u0026#34;Sophie\u0026#39;s World\u0026#34; }, \u0026#34;gsx$tags\u0026#34;: { \u0026#34;$t\u0026#34;: \u0026#34;philosophy\u0026#34; }, \u0026#34;gsx$insertdate\u0026#34;: { \u0026#34;$t\u0026#34;: \u0026#34;22/04/2020\u0026#34; } }, ... ] } } We can see that the data can be found under data.feed.entry. Values in each cell of different columns can be retrieved by using a key in the format of gsx${column_name}, such as gsx$quote.\nVue.js for Frontend Development Vue.js is a JavaScript framework for building Web-based user interfaces and applications. In this project, it was used to develop the Web page that displays the quotations stored in the Google Sheet.\nFirstly, follow the Vue.js official guide to include the necessary JavaScript file in the HTML source code:\n\u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/vue\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; In the Web page, we have an element representing the whole application:\n\u0026lt;div id=\u0026#34;app\u0026#34;\u0026gt; \u0026lt;/div\u0026gt; The following script is that used to load the content from the Google Sheet once the page is loaded:\nvar gsheet_url = \u0026#34;{{url_to_the_google_sheet_json}}\u0026#34; var app = new Vue({ el: \u0026#39;#app\u0026#39;, data () { return { info: null } }, mounted () { axios .get(gsheet_url) .then(response =\u0026gt; ( parseData(response.data.feed.entry) )) } }); The parseData function will take the list of entries in the JSON data, and store the quotations in a local data structure for display:\n// Variables to hold the parsed data var quoteList = []; var authorList = []; var tagList = []; function parseData(entries) { var authorSet = new Set(); var tagSet = new Set(); entries.forEach(function(value) { var entry = { \u0026#34;quote\u0026#34;: value.gsx$quote.$t, \u0026#34;author\u0026#34;: value.gsx$author.$t, \u0026#34;source\u0026#34;: value.gsx$source.$t, \u0026#34;tags\u0026#34;: value.gsx$tags.$t.split(\u0026#34;,\u0026#34;) }; // Add to the set of authors authorSet.add(entry.author); // Add to the set of tags entry.tags.forEach(function(t) { tagSet.add(t); }); // Push entry into the list of quotes quoteList.push(entry); }); authorList = Array.from(authorSet); authorList.sort(); tagList = Array.from(tagSet); tagList.sort(); } Finally, we can display the data using the template syntax provided by Vue.js. For example, the list of quotations are displayed using the following template:\n\u0026lt;div id=\u0026#34;quotes\u0026#34;\u0026gt; \u0026lt;div v-for=\u0026#34;quote in quotes\u0026#34; class=\u0026#34;quote\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;quote-text\u0026#34;\u0026gt;\u0026#34;{{ quote.quote }}\u0026#34;\u0026lt;/div\u0026gt; \u0026lt;div\u0026gt; \u0026lt;span class=\u0026#34;author\u0026#34;\u0026gt;- {{ quote.author}}\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;source\u0026#34; v-if=\u0026#34;quote.source != \u0026#39;\u0026#39;\u0026#34;\u0026gt;({{ quote.source }})\u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;tag-container\u0026#34;\u0026gt; \u0026lt;span v-for=\u0026#34;tag in quote.tags\u0026#34; class=\u0026#34;tag\u0026#34;\u0026gt; {{ tag }} \u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; var quotes = new Vue({ el: \u0026#39;#quotes\u0026#39;, data: { quotes: quoteList } }); In this project, I also tried to implement some simple filters on the Web app to allow user to quickly filter quotations by the author names or the tags assigned to them. For details, you can refer to the source code on github.\nOverall, I found this combination very useful when we want to quickly build a user interface for browsing structured data stored in a spreadsheet. This avoids the need to create a database and a backend application. I can see a lot of potentials in using this method to quickly create simple and interesting applications.\n","date":"2020-04-26T00:00:00Z","permalink":"https://albertauyeung.github.io/2020/04/26/vuejs-google-sheets.html","title":"📊 Build Your Dynamic Website Easily with Vue.js and Google Sheets"},{"content":"Matplotlib by default does not support displaying Unicode characters such as Chinese, Japanese and Korean characters. This post introduces two different methods to allow these characters to be shown in the graphs.\nThe issue here is that we need to configure Matplotlib to use fonts that support the characters that we want to display. To configure the font used by Matplotlib, there are two ways.\nSpecifying the Path to the Font File If you have a font file that support displaying CJK characters, you can directly provide the path to the font file using the FontProperties class in Matplotlib. This font file can be a .ttf file (TrueType Font) or a .otf file (OpenType Font). For example, you can download a OTF font that supports displaying CJK characters from Google Fonts.\nOnce we have the font file, we can create a FontProperties instance as follows:\nimport matplotlib.font_manager as fm fprop = fm.FontProperties(fname=\u0026#39;NotoSansCJKtc-Regular.otf\u0026#39;) When plotting a graph, we can provide this FonProperties instance as an argument to functions that control what words are displayed in the graph. The example below shows how to set the font for the title and the labels on the X-axis.\nimport matplotlib.pyplot as plt # Prepare some data x = list(range(20)) xticks = [\u0026#34;類別{:d}\u0026#34;.format(i) for i in x] y = [random.randint(10,99) for i in x] # Plot the graph plt.figure(figsize=(8, 2)) plt.bar(x, y) plt.xticks(x, xticks, fontproperties=fprop, fontsize=12, rotation=45) plt.title(\u0026#34;圖1\u0026#34;, fontproperties=fprop, fontsize=18) plt.show() The effect will be as follows:\nUsing Fonts in the Font Folder Another way of using a custom font is to install the font into Matplotlib\u0026rsquo;s font folder, and update the font manager.\nFirstly, we need to know the path to the font folder. We can first use the following command to check the location of the Matplotlib installation:\nprint(matplotlib.matplotlib_fname()) On my computer the above command will print:\n/usr/local/lib/python3.7/site-packages/matplotlib/mpl-data/matplotlibrc The full path to the font folder can be obtained by replacing /mpl-data/matplotlibrc with /mpl-data/fonts/ttf:\n/usr/local/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf Next, you can put the font file you would like to use into the font folder, and then update Matplotlib\u0026rsquo;s font manager:\nimport matplotlib.font_manager as fm fm._rebuild() Once this is done, you can check the name of the font you have installed using the following statement. In this example, I have downloaded the font NotoSansCJKtc-Regular.otf from Google Font, and placed it in the file folder.\n[f for f in fm.fontManager.ttflist if \u0026#39;Noto\u0026#39; in f.name] And the following is the output:\n[\u0026lt;Font \u0026#39;Noto Sans CJK TC\u0026#39; (NotoSansCJKtc-Regular.otf) normal normal 400 normal\u0026gt;, \u0026lt;Font \u0026#39;Noto Sans Tagalog\u0026#39; (NotoSansTagalog-Regular.ttf) normal normal 400 normal\u0026gt;, \u0026lt;Font \u0026#39;Noto Sans Kayah Li\u0026#39; (NotoSansKayahLi-Regular.ttf) normal normal 400 normal\u0026gt;, \u0026lt;Font \u0026#39;Noto Sans Tai Tham\u0026#39; (NotoSansTaiTham-Regular.ttf) normal normal 400 normal\u0026gt;, \u0026lt;Font \u0026#39;Noto Sans Ol Chiki\u0026#39; (NotoSansOlChiki-Regular.ttf) normal normal 400 normal\u0026gt;, ... Here, the name of the font is \u0026ldquo;Noto Sans CJK TC\u0026rdquo;. We can then configure Matplotlib to use this font in our graphs:\nmatplotlib.rcParams[\u0026#39;font.family\u0026#39;] = [\u0026#39;Noto Sans CJK TC\u0026#39;] Below is an example:\nplt.figure(figsize=(8, 2)) plt.bar(x, y) plt.xticks(x, xticks, fontsize=12, rotation=45) plt.title(\u0026#34;圖1\u0026#34;, fontsize=18) plt.show() which will produce the same graph as above:\nUsing Custom Fonts in Seaborn Choosing the second method described above allowing you to use the font in Seaborn too. Below is an example that shows how you can configure the font to be used in Seaborn.\nimport seaborn as sns colour = sns.color_palette(\u0026#34;GnBu_d\u0026#34;) sns.set(rc={\u0026#39;figure.figsize\u0026#39;:(8, 2), \u0026#39;figure.dpi\u0026#39;:120}) sns.set(font=\u0026#39;Noto Sans CJK TC\u0026#39;) ax = sns.barplot(xticks, y, palette=colour) ax.set_xticklabels(xticks, rotation=45, fontsize=9) ax.set_title(\u0026#34;圖表1\u0026#34;) ax.grid() And the following graph will be produced:\n","date":"2020-03-15T00:00:00Z","permalink":"https://albertauyeung.github.io/2020/03/15/matplotlib-cjk-fonts.html","title":"🌐 A Guide to Displaying CJK Characters in Matplotlib"},{"content":"PyCon HK 2018 was held on 23-24th November 2018 at Cyberport. I gave a talk on how to deploy machine learning models in Python. The slides of the talk can be found at the link: http://talks.albertauyeung.com/pycon2018-deploy-ml-models/.\nVideo on Youtube ","date":"2018-11-23T00:00:00Z","permalink":"https://albertauyeung.github.io/2018/11/23/pyconhk-ml-deploy.html","title":"🚀 Deploying ML Models in Python - A PyCon HK 2018 Talk"},{"content":"N-grams are contiguous sequences of n-items in a sentence. N can be 1, 2 or any other positive integers, although usually we do not consider very large N because those n-grams rarely appears in many different places.\nWhen performing machine learning tasks related to natural language processing, we usually need to generate n-grams from input sentences. For example, in text classification tasks, in addition to using each individual token found in the corpus, we may want to add bi-grams or tri-grams as features to represent our documents. This post describes several different ways to generate n-grams quickly from input sentences in Python.\nThe Pure Python Way In general, an input sentence is just a string of characters in Python. We can use build in functions in Python to generate n-grams quickly. Let\u0026rsquo;s take the following sentence as a sample input:\ns = \u0026#34;\u0026#34;\u0026#34; Natural-language processing (NLP) is an area of computer science and artificial intelligence concerned with the interactions between computers and human (natural) languages. \u0026#34;\u0026#34;\u0026#34; If we want to generate a list of bi-grams from the above sentence, the expected output would be something like below (depending on how do we want to treat the punctuations, the desired output can be different):\n[ \u0026#34;natural language\u0026#34;, \u0026#34;language processing\u0026#34;, \u0026#34;processing nlp\u0026#34;, \u0026#34;nlp is\u0026#34;, \u0026#34;is an\u0026#34;, \u0026#34;an area\u0026#34;, ... ] The following function can be used to achieve this:\nimport re def generate_ngrams(s, n): # Convert to lowercases s = s.lower() # Replace all none alphanumeric characters with spaces s = re.sub(r\u0026#39;[^a-zA-Z0-9\\s]\u0026#39;, \u0026#39; \u0026#39;, s) # Break sentence in the token, remove empty tokens tokens = [token for token in s.split(\u0026#34; \u0026#34;) if token != \u0026#34;\u0026#34;] # Use the zip function to help us generate n-grams # Concatentate the tokens into ngrams and return ngrams = zip(*[token[i:] for i in range(n)]) return [\u0026#34; \u0026#34;.join(ngram) for ngram in ngrams] Applying the above function to the sentence, with n=5, gives the following output:\n\u0026gt;\u0026gt;\u0026gt; generate_ngrams(s, n=5) [\u0026#39;natural language processing nlp is\u0026#39;, \u0026#39;language processing nlp is an\u0026#39;, \u0026#39;processing nlp is an area\u0026#39;, \u0026#39;nlp is an area of\u0026#39;, \u0026#39;is an area of computer\u0026#39;, \u0026#39;an area of computer science\u0026#39;, \u0026#39;area of computer science and\u0026#39;, \u0026#39;of computer science and artificial\u0026#39;, \u0026#39;computer science and artificial intelligence\u0026#39;, \u0026#39;science and artificial intelligence concerned\u0026#39;, \u0026#39;and artificial intelligence concerned with\u0026#39;, \u0026#39;artificial intelligence concerned with the\u0026#39;, \u0026#39;intelligence concerned with the interactions\u0026#39;, \u0026#39;concerned with the interactions between\u0026#39;, \u0026#39;with the interactions between computers\u0026#39;, \u0026#39;the interactions between computers and\u0026#39;, \u0026#39;interactions between computers and human\u0026#39;, \u0026#39;between computers and human natural\u0026#39;, \u0026#39;computers and human natural languages\u0026#39;] The above function makes use of the zip function, which creates a generator that aggregates elements from multiple lists (or iterables in genera). The blocks of codes and comments below offer some more explanation of the usage:\n# Sample sentence s = \u0026#34;one two three four five\u0026#34; tokens = s.split(\u0026#34; \u0026#34;) # tokens = [\u0026#34;one\u0026#34;, \u0026#34;two\u0026#34;, \u0026#34;three\u0026#34;, \u0026#34;four\u0026#34;, \u0026#34;five\u0026#34;] sequences = [tokens[i:] for i in range(3)] # The above will generate sequences of tokens starting # from different elements of the list of tokens. # The parameter in the range() function controls # how many sequences to generate. # # sequences = [ # [\u0026#39;one\u0026#39;, \u0026#39;two\u0026#39;, \u0026#39;three\u0026#39;, \u0026#39;four\u0026#39;, \u0026#39;five\u0026#39;], # [\u0026#39;two\u0026#39;, \u0026#39;three\u0026#39;, \u0026#39;four\u0026#39;, \u0026#39;five\u0026#39;], # [\u0026#39;three\u0026#39;, \u0026#39;four\u0026#39;, \u0026#39;five\u0026#39;]] bigrams = zip(*sequences) # The zip function takes the sequences as a list of inputs # (using the * operator, this is equivalent to # zip(sequences[0], sequences[1], sequences[2]). # Each tuple it returns will contain one element from # each of the sequences. # # To inspect the content of bigrams, try: # print(list(bigrams)) # which will give the following: # # [ # (\u0026#39;one\u0026#39;, \u0026#39;two\u0026#39;, \u0026#39;three\u0026#39;), # (\u0026#39;two\u0026#39;, \u0026#39;three\u0026#39;, \u0026#39;four\u0026#39;), # (\u0026#39;three\u0026#39;, \u0026#39;four\u0026#39;, \u0026#39;five\u0026#39;) # ] # # Note: even though the first sequence has 5 elements, # zip will stop after returning 3 tuples, because the # last sequence only has 3 elements. In other words, # the zip function automatically handles the ending of # the n-gram generation. Using NLTK Instead of using pure Python functions, we can also get help from some natural language processing libraries such as the Natural Language Toolkit (NLTK). In particular, nltk has the ngrams function that returns a generator of n-grams given a tokenized sentence. (See the documentaion of the function here)\nimport re from nltk.util import ngrams s = s.lower() s = re.sub(r\u0026#39;[^a-zA-Z0-9\\s]\u0026#39;, \u0026#39; \u0026#39;, s) tokens = [token for token in s.split(\u0026#34; \u0026#34;) if token != \u0026#34;\u0026#34;] output = list(ngrams(tokens, 5)) The above block of code will generate the same output as the function generate_ngrams() as shown above.\n","date":"2018-06-03T00:00:00Z","permalink":"https://albertauyeung.github.io/2018/06/03/generating-ngrams.html","title":"🐍 Effortlessly Create N-Grams from Text in Python"},{"content":"PyCon HK 2017 was held on 3rd-4th November 2017 at the City University of Hong Kong. I gave a talk on using gradient boosting machines in Python to perform machine learning. The slides of the talk can be found at the link: http://talks.albertauyeung.com/pycon2017-gradient-boosting/.\nVideo on Youtube ","date":"2017-11-05T00:00:00Z","permalink":"https://albertauyeung.github.io/2017/11/05/pyconhk-gbm.html","title":"🚀 Using Gradient Boosting Machines in Python - A PyCon HK 2017 Talk"},{"content":"I gave a talk on deep learning and its applications in a research seminar at the Deep Learning Research \u0026amp; Application Centre (DLC), Hang Seng Management College on 20th July, 2017. The slides of the talk can be found here: http://talks.albertauyeung.com/deep-learning\n","date":"2017-07-21T00:00:00Z","permalink":"https://albertauyeung.github.io/2017/07/21/hsmc-deep-learning-talk.html","title":"🤖 Deep Learning and Its Applications - Research Seminar at HSMC"},{"content":"pandas is one of the most commonly used Python library in data analysis and machine learning. It is versatile and can be used to handle many different types of data. Before feeding a model with training data, one would most probably pre-process the data and perform feature extraction on data stored as pandas DataFrame. I have been using pandas extensively in my work, and have recently discovered that the time required to manipulate data stored in a DataFrame can vary hugely depending on the method you used.\nNumerical Operations To demonstrate the differences, let\u0026rsquo;s generate some random data first. The following block of code will generate a DataFrame with 5,000 rows and 3 columns (A, B and C) with values ranging from -10 to 10.\nIn [1]: import pandas as pd In [2]: import numpy as np In [3]: data = np.random.randint(-10, 10, (5000, 3)) In [4]: df = pd.DataFrame(data=data, columns=[\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;], index=None) To track the time required to finish an operation, we can make use of the IPython magic function %timeit to measure the time required to execute a line in Python.\nTo start with, let\u0026rsquo;s consider a simple task of creating a new column in the DataFrame, whose values depend on whether the sum of the values in other columns are greater than zero. First, let\u0026rsquo;s try using the apply function of the DataFrame:\nIn [5]: %timeit df[\u0026#34;D\u0026#34;] = df.apply(lambda x: 1 if x[\u0026#34;A\u0026#34;] + x[\u0026#34;B\u0026#34;] + x[\u0026#34;C\u0026#34;] \u0026gt; 0 else 0, axis=1) 134 ms ± 1.59 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) It takes about 134ms to finish the operation, which seems quite fast. However, if we take another approach by using numpy\u0026rsquo;s where() function, we can actually be much faster:\nIn [6]: %timeit df[\u0026#34;E\u0026#34;] = np.where(df[\u0026#34;A\u0026#34;] + df[\u0026#34;B\u0026#34;] + df[\u0026#34;C\u0026#34;] \u0026gt; 0, 1, 0) 757 µs ± 38.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) This is ~170 times faster! We can verified that the two methods actually give the same results as follows. (np.any checks if any of the values in a list is True).\nIn [7]: np.any(df[\u0026#34;D\u0026#34;] != df[\u0026#34;E\u0026#34;]) False String Operations As another example, let\u0026rsquo;s try searching substrings in a column. Firstly, let\u0026rsquo;s generate some random text data in a new column:\nIn [8]: df[\u0026#34;F\u0026#34;] = np.random.choice([\u0026#34;apple\u0026#34;, \u0026#34;banana\u0026#34;, \u0026#34;orange\u0026#34;, \u0026#34;pear\u0026#34;], 5000) Let\u0026rsquo;s say we want to create a new column, whose values depend on whether Column F contains the substring an. Firstly, let\u0026rsquo;s try the apply function:\nIn [9]: %timeit df[\u0026#34;G\u0026#34;] = df.apply(lambda x: 1 if \u0026#34;an\u0026#34; in x[\u0026#34;F\u0026#34;] else 0, axis=1) 61.1 ms ± 685 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) Now, if we use the second approach:\nIn [10]: %timeit df[\u0026#34;H\u0026#34;] = np.where(df[\u0026#34;F\u0026#34;].str.contains(\u0026#34;an\u0026#34;), 1, 0) 2.65 ms ± 40.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) which is ~30 times faster.\nThe conclusion is that whenever we can operate on the whole column, we should avoid using apply, which is looping over every row of the DataFrame, and is not able to take advantage of numpy vectorization when performing the calculation.\n","date":"2017-07-08T00:00:00Z","permalink":"https://albertauyeung.github.io/2017/07/08/fast-pandas-operation.html","title":"⚡ Making pandas Operations Faster"},{"content":"Sequence Labelling in NLP In natural language processing, it is a common task to extract words or phrases of particular types from a given sentence or paragraph. For example, when performing analysis of a corpus of news articles, we may want to know which countries are mentioned in the articles, and how many articles are related to each of these countries.\nThis is actually a special case of sequence labelling in NLP (others include POS tagging and Chunking), in which the goal is to assign a label to each member in the sequence. In the case of identifying country names, we would like to assign a \u0026lsquo;country\u0026rsquo; label to words that form part of a country name, and a \u0026lsquo;irrelevant\u0026rsquo; label to all other words. For example, the following is a sentence broken down into tokens, and its desired output after the sequence labelling process:\ninput = [\u0026#34;Paris\u0026#34;, \u0026#34;is\u0026#34;, \u0026#34;the\u0026#34;, \u0026#34;capital\u0026#34;, \u0026#34;of\u0026#34;, \u0026#34;France\u0026#34;] output = [\u0026#34;I\u0026#34;, \u0026#34;I\u0026#34;, \u0026#34;I\u0026#34;, \u0026#34;I\u0026#34;, \u0026#34;I\u0026#34;, \u0026#34;C\u0026#34;] where I means that the token of that position is an irrelevant word, and C means that the token of that position is a word that form part of a country name.\nMethods of Sequence Labelling A simple, though sometimes quite useful, approach is to prepare a dictionary of country names, and look for these names in each of the sentences in the corpus. However, this method relies heavily on the comprehensiveness of the dictionary. While there is a limited number of countries, in other cases such as city names the number of possible entries in the dictionary can be huge. Even for countries, many countries may be referred to using different sequence of characters in different contexts. For example, the United States of America may be referred to in an article as the USA, the States, or simply America.\nIn fact, a person reading a news article would usually recognise that a word or a phrase refers to a country, even when he or she has not seen the name of that country before. The reason is that there are many differnt cues in the sentence or the whole article that can be used to determine whether a word or a phrase is a country name. Take the following two sentences as examples:\nKerry travels to Laos\u0026rsquo;s capital, Vientiane, on Monday for meetings of foreign ministers from the 10-member Association of South East Asia Nations (ASEAN). The Governments of Bolivia and Uruguay will strengthen ties with a customs cooperation agreement to be in force on June 15th. The first sentence implies that something called Lao has a capital, suggesting that Lao is a country. Similarly, in the second sentence we know that both Bolivia and Uruguay are countries as the news mentioned about their governments. In other words, the words around \u0026lsquo;Lao\u0026rsquo;, \u0026lsquo;Bolivia\u0026rsquo; and \u0026lsquo;Uruguay\u0026rsquo; provide clues as to whether they are country names.\nConditional Random Field (CRF) To take advantage of the surrounding context when labelling tokens in a sequence, a commonly used method is conditional random field (CRF), first proposed by Lafferty et al. in 2001. It is a type of probabilistic graphical model that can be used to model sequential data, such as labels of words in a sentence.\nThis article is not intended to discuss the technical details of CRF. If you are interested, you are recommended to check out one of the following tutorials which provide very good explanation of how CRF works:\nAn Introduction to Conditional Random Fields by Charles Sutton and Andrew McCallum Introduction to Conditional Random Fields by Edwin Chen In CRF, we will design a set of feature functions to extract features for each word in a sentence. During model training, CRF will try to determine the weights of different feature functions that will maximise the likelihood of the labels in the training data.\nTrain CRF Model in Python One of the commonly used CRF library is CRFSuite implemented by Naoaki Okazaki in C/C++. The library is already easy to use given its command line interface. A Python binding to CRFSuite, pycrfsuite is available for using the API in Python. This Python module is exactly the module used in the POS tagger in the nltk module.\nTo demonstrate how pysrfsuite can be used to train a linear chained CRF sequence labelling model, we will go through an example using some data for named entity recognition.\nNamed Entity Recogniton To train a named entity recognition model, we need some labelled data. The dataset that will be used below is the Reuters-128 dataset, which is an English corpus in the NLP Interchange Format (NIF). It contains 128 economic news articles. The dataset contains information for 880 named entities with their position in the document and a URI of a DBpedia resource identifying the entity. It was created by the Agile Knowledge Engineering and Semantic Web research group at Leipzig University, Germany. More details can be found in their paper.\nIn the following, we will use the XML verison of the dataset, which can be downloaded from https://github.com/AKSW/n3-collection. Below is some lines extracted from the XML data file:\n\u0026lt;document id=\u0026#34;8\u0026#34;\u0026gt; \u0026lt;documenturi\u0026gt;http://www.research.att.com/~lewis/Reuters-21578/15009\u0026lt;/documenturi\u0026gt; \u0026lt;documentsource\u0026gt;Reuters-21578\u0026lt;/documentsource\u0026gt; \u0026lt;textwithnamedentities\u0026gt; \u0026lt;namedentityintext uri=\u0026#34;http://aksw.org/notInWiki/Home_Intensive_Care_Inc\u0026#34;\u0026gt;Home Intensive Care Inc\u0026lt;/namedentityintext\u0026gt; \u0026lt;simpletextpart\u0026gt; said it has opened a Dialysis at Home office in \u0026lt;/simpletextpart\u0026gt; \u0026lt;namedentityintext uri=\u0026#34;http://dbpedia.org/resource/Philadelphia\u0026#34;\u0026gt;Philadelphia\u0026lt;/namedentityintext\u0026gt; \u0026lt;simpletextpart\u0026gt;, its 12th nationwide.\u0026lt;/simpletextpart\u0026gt; \u0026lt;/textwithnamedentities\u0026gt; \u0026lt;/document\u0026gt; The XML block shown above refers to one of the documents in the dataset. The semantics is self-explanatory. The document has a sentence \u0026lsquo;Home Intensive Care Inc said it has opened a Dialysis at Home office in Philadelphia, its 12th nationwide\u0026rsquo;, in which Home Intensive Care Inc and Philadelphia are labelled as named entities.\nPrepare the Dataset for Training In order to prepare the dataset for training, we need to label every word (or token) in the sentences to be either irrelevant or part of a named entity. Since the data is in XML format, we can make use of BeautifulSoup to parse the file and extract the data as follows:\nfrom bs4 import BeautifulSoup as bs from bs4.element import Tag import codecs # Read data file and parse the XML with codecs.open(\u0026#34;reuters.xml\u0026#34;, \u0026#34;r\u0026#34;, \u0026#34;utf-8\u0026#34;) as infile: soup = bs(infile, \u0026#34;html5lib\u0026#34;) docs = [] for elem in soup.find_all(\u0026#34;document\u0026#34;): texts = [] # Loop through each child of the element under \u0026#34;textwithnamedentities\u0026#34; for c in elem.find(\u0026#34;textwithnamedentities\u0026#34;).children: if type(c) == Tag: if c.name == \u0026#34;namedentityintext\u0026#34;: label = \u0026#34;N\u0026#34; # part of a named entity else: label = \u0026#34;I\u0026#34; # irrelevant word for w in c.text.split(\u0026#34; \u0026#34;): if len(w) \u0026gt; 0: texts.append((w, label)) docs.append(texts) The result will be a list of documents, each of which contains a list of (word, label) tuples. For example:\n\u0026gt;\u0026gt;\u0026gt; doc[0][:10] [(\u0026#39;Paxar\u0026#39;, \u0026#39;N\u0026#39;), (\u0026#39;Corp\u0026#39;, \u0026#39;N\u0026#39;), (\u0026#39;said\u0026#39;, \u0026#39;I\u0026#39;), (\u0026#39;it\u0026#39;, \u0026#39;I\u0026#39;), (\u0026#39;has\u0026#39;, \u0026#39;I\u0026#39;), (\u0026#39;acquired\u0026#39;, \u0026#39;I\u0026#39;), (\u0026#39;Thermo-Print\u0026#39;, \u0026#39;N\u0026#39;), ... Generating Part-of-Speech Tags To train a CRF model, we need to create features for each of the tokens in the sentences. One particularly useful feature in NLP is the part-of-speech (POS) tags of the words. They indicates whether a word is a noun, a verb or an adjective. (In fact, a POS tagger is also usually a trained CRF model.)\nWe can use NLTK\u0026rsquo;s POS tagger to generate the POS tags for the tokens in our documents as follows:\nimport nltk data = [] for i, doc in enumerate(docs): # Obtain the list of tokens in the document tokens = [t for t, label in doc] # Perform POS tagging tagged = nltk.pos_tag(tokens) # Take the word, POS tag, and its label data.append([(w, pos, label) for (w, label), (word, pos) in zip(doc, tagged)]) The output of the above process will be a list of documents, each of which is a list of tuples with the word, its POS tag and its label:\n\u0026gt;\u0026gt;\u0026gt; data[0] [(\u0026#39;Paxar\u0026#39;, \u0026#39;NNP\u0026#39;, \u0026#39;N\u0026#39;), (\u0026#39;Corp\u0026#39;, \u0026#39;NNP\u0026#39;, \u0026#39;N\u0026#39;), (\u0026#39;said\u0026#39;, \u0026#39;VBD\u0026#39;, \u0026#39;I\u0026#39;), (\u0026#39;it\u0026#39;, \u0026#39;PRP\u0026#39;, \u0026#39;I\u0026#39;), (\u0026#39;has\u0026#39;, \u0026#39;VBZ\u0026#39;, \u0026#39;I\u0026#39;), (\u0026#39;acquired\u0026#39;, \u0026#39;VBN\u0026#39;, \u0026#39;I\u0026#39;), (\u0026#39;Thermo-Print\u0026#39;, \u0026#39;NNP\u0026#39;, \u0026#39;N\u0026#39;), ... Generating Features Given the POS tags, we can now continue to generate more features for each of the tokens in the dataset. The features that will be useful in the training process depends on the task at hand. Below are some of the commonly used features for a word $w$ in named entity recognition:\nThe word $w$ itself (converted to lowercase for normalisation) The prefix/suffix of $w$ (e.g. -ion) The words surrounding $w$, such as the previous and the next word Whether $w$ is in uppercase or lowercase Whether $w$ is a number, or contains digits The POS tag of $w$, and those of the surrounding words Whether $w$ is or contains a special character (e.g. hypen, dollar sign) Below is a function for generating features for our documents. It takes a doc (in the form of a listof tuples as shown above), and an index (the $i$th document), and return the documents with features extracted. (A similar example can be found in the repository of pyscrfsuite.)\ndef word2features(doc, i): word = doc[i][0] postag = doc[i][1] # Common features for all words features = [ \u0026#39;bias\u0026#39;, \u0026#39;word.lower=\u0026#39; + word.lower(), \u0026#39;word[-3:]=\u0026#39; + word[-3:], \u0026#39;word[-2:]=\u0026#39; + word[-2:], \u0026#39;word.isupper=%s\u0026#39; % word.isupper(), \u0026#39;word.istitle=%s\u0026#39; % word.istitle(), \u0026#39;word.isdigit=%s\u0026#39; % word.isdigit(), \u0026#39;postag=\u0026#39; + postag ] # Features for words that are not # at the beginning of a document if i \u0026gt; 0: word1 = doc[i-1][0] postag1 = doc[i-1][1] features.extend([ \u0026#39;-1:word.lower=\u0026#39; + word1.lower(), \u0026#39;-1:word.istitle=%s\u0026#39; % word1.istitle(), \u0026#39;-1:word.isupper=%s\u0026#39; % word1.isupper(), \u0026#39;-1:word.isdigit=%s\u0026#39; % word1.isdigit(), \u0026#39;-1:postag=\u0026#39; + postag1 ]) else: # Indicate that it is the \u0026#39;beginning of a document\u0026#39; features.append(\u0026#39;BOS\u0026#39;) # Features for words that are not # at the end of a document if i \u0026lt; len(doc)-1: word1 = doc[i+1][0] postag1 = doc[i+1][1] features.extend([ \u0026#39;+1:word.lower=\u0026#39; + word1.lower(), \u0026#39;+1:word.istitle=%s\u0026#39; % word1.istitle(), \u0026#39;+1:word.isupper=%s\u0026#39; % word1.isupper(), \u0026#39;+1:word.isdigit=%s\u0026#39; % word1.isdigit(), \u0026#39;+1:postag=\u0026#39; + postag1 ]) else: # Indicate that it is the \u0026#39;end of a document\u0026#39; features.append(\u0026#39;EOS\u0026#39;) return features Training the Model To train the model, we need to first prepare the training data and the corresponding labels. Also, to be able to investigate the accuracy of the model, we need to separate the data into training set and test set. Below are some codes for preparing the training data and test data, using the train_test_split function in scikit-learn.\nfrom sklearn.model_selection import train_test_split # A function for extracting features in documents def extract_features(doc): return [word2features(doc, i) for i in range(len(doc))] # A function fo generating the list of labels for each document def get_labels(doc): return [label for (token, postag, label) in doc] X = [extract_features(doc) for doc in data] y = [get_labels(doc) for doc in data] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) In pycrfsuite, A CRF model in can be trained by first creating a trainer, and then submit the training data and corresponding labels to the trainer. After that, set the parameters and call train() to start the training process. For the complete list of parameters, one can refer to the documentation of CRFSuite. With the very small dataset in this example, the training with max_iterations=200 can be finished in a few seconds. Below is the code for creating the trainer and start training the model:\nimport pycrfsuite trainer = pycrfsuite.Trainer(verbose=True) # Submit training data to the trainer for xseq, yseq in zip(X_train, y_train): trainer.append(xseq, yseq) # Set the parameters of the model trainer.set_params({ # coefficient for L1 penalty \u0026#39;c1\u0026#39;: 0.1, # coefficient for L2 penalty \u0026#39;c2\u0026#39;: 0.01, # maximum number of iterations \u0026#39;max_iterations\u0026#39;: 200, # whether to include transitions that # are possible, but not observed \u0026#39;feature.possible_transitions\u0026#39;: True }) # Provide a file name as a parameter to the train function, such that # the model will be saved to the file when training is finished trainer.train(\u0026#39;crf.model\u0026#39;) If you have set verbose=True when initialising the trainer, the trainer will print out the training progress as it is trained against the provided training data.\nChecking the Results Once we have the model trained, we can apply it on our test data and see whether it gives reasonable results. Assuming that the model is saved to a file named crf.model. The following block of code shows how we can load the model into memory, and apply it on to our test data.\ntagger = pycrfsuite.Tagger() tagger.open(\u0026#39;crf.model\u0026#39;) y_pred = [tagger.tag(xseq) for xseq in X_test] # Let\u0026#39;s take a look at a random sample in the testing set i = 12 for x, y in zip(y_pred[i], [x[1].split(\u0026#34;=\u0026#34;)[1] for x in X_test[i]]): print(\u0026#34;%s (%s)\u0026#34; % (y, x)) \u0026#39;\u0026#39;\u0026#39; The following will be printed: sci-med (N) life (N) systems (N) inc (N) said (I) its (I) directors (I) approved (I) a (I) previously (I) ... \u0026#39;\u0026#39;\u0026#39; The result looks reasonable as the first four words are correctly identified as part of a named entity.\nTo study the performance of the CRF tagger trained above in a more quantitative way, we can check the precision and recall on the test data. This can be done very easily using the classification_report function in scikit-learn. However, given that the predictions are sequences of tags, we need to transform the data into a list of labels before feeding them into the function.\nimport numpy as np from sklearn.metrics import classification_report # Create a mapping of labels to indices labels = {\u0026#34;N\u0026#34;: 1, \u0026#34;I\u0026#34;: 0} # Convert the sequences of tags into a 1-dimensional array predictions = np.array([labels[tag] for row in y_pred for tag in row]) truths = np.array([labels[tag] for row in y_test for tag in row]) # Print out the classification report print(classification_report( truths, predictions, target_names=[\u0026#34;I\u0026#34;, \u0026#34;N\u0026#34;])) which will prints a report as follows:\nprecision recall f1-score support I 0.98 0.98 0.98 3322 N 0.85 0.85 0.85 405 avg / total 0.97 0.97 0.97 3727 We can see that we have achieved 85% precision and 85% recall in predicting whether a word is part of a named entity. There are several things by which we can improve the performance, including creating better features or tuning the parameters of the CRF model.\nSource Codes The source code for reproducing the above results can be found in the following github repository: https://github.com/albertauyeung/python-crf-named-entity-recognition.\nReferences Lafferty, J., McCallum, A., Pereira, F. (2001). \u0026ldquo;Conditional random fields: Probabilistic models for segmenting and labeling sequence data\u0026rdquo;. Proc. 18th International Conf. on Machine Learning. Morgan Kaufmann. pp. 282–289. Erdogan, H. (2010). Sequence Labeling: Generative and Discriminative Approaches - Hidden Markov Models, Conditional Random Fields and Structured SVMs. ICMLA 2010 Tutorial. ","date":"2017-06-17T00:00:00Z","permalink":"https://albertauyeung.github.io/2017/06/17/python-sequence-labelling-with-crf.html","title":"🔍 Performing Sequence Labelling using CRF in Python"},{"content":"There is probably no need to say that there is too much information on the Web nowadays. Search engines help us a little bit. What is better is to have something interesting recommended to us automatically without asking. Indeed, from as simple as a list of the most popular questions and answers on Quora to some more personalized recommendations we received on Amazon, we are usually offered recommendations on the Web.\nRecommendations can be generated by a wide range of algorithms. While user-based or item-based collaborative filtering methods are simple and intuitive, matrix factorization techniques are usually more effective because they allow us to discover the latent features underlying the interactions between users and items. Of course, matrix factorization is simply a mathematical tool for playing around with matrices, and is therefore applicable in many scenarios where one would like to find out something hidden under the data.\nIn this tutorial, we will go through the basic ideas and the mathematics of matrix factorization, and then we will present a simple implementation in Python. We will proceed with the assumption that we are dealing with user ratings (e.g. an integer score from the range of 1 to 5) of items in a recommendation system.\nBasic Idea Just as its name suggests, matrix factorization is to, obviously, factorize a matrix, i.e. to find out two (or more) matrices such that when you multiply them you will get back the original matrix.\nAs mentioned above, from an application point of view, matrix factorization can be used to discover latent features underlying the interactions between two different kinds of entities. (Of course, you can consider more than two kinds of entities and you will be dealing with tensor factorization, which would be more complicated.) And one obvious application is to predict ratings in collaborative filtering.\nIn a recommendation system such as Netflix or MovieLens, there is a group of users and a set of items (movies for the above two systems). Given that each users have rated some items in the system, we would like to predict how the users would rate the items that they have not yet rated, such that we can make recommendations to the users. In this case, all the information we have about the existing ratings can be represented in a matrix. Assume now we have 5 users and 10 items, and ratings are integers ranging from 1 to 5, the matrix may look something like this (a hyphen means that the user has not yet rated the movie):\nD1 D2 D3 D4 U1 5 3 - 1 U2 4 - - 1 U3 1 1 - 5 U4 1 - - 4 U5 - 1 5 4 Hence, the task of predicting the missing ratings can be considered as filling in the blanks (the hyphens in the matrix) such that the values would be consistent with the existing ratings in the matrix.\nThe intuition behind using matrix factorization to solve this problem is that there should be some latent features that determine how a user rates an item. For example, two users would give high ratings to a certain movie if they both like the actors or actresses in the movie, or if the movie is an action movie, which is a genre preferred by both users.\nHence, if we can discover these latent features, we should be able to predict a rating with respect to a certain user and a certain item, because the features associated with the user should match with the features associated with the item.\nIn trying to discover the different features, we also make the assumption that the number of features would be smaller than the number of users and the number of items. It should not be difficult to understand this assumption because clearly it would not be reasonable to assume that each user is associated with a unique feature (although this is not impossible). And anyway if this is the case there would be no point in making recommendations, because each of these users would not be interested in the items rated by other users. Similarly, the same argument applies to the items.\nThe Maths of Matrix Factorization Having discussed the intuition behind matrix factorization, we can now go on to work on the mathematics. Firstly, we have a set $U$ of users, and a set $D$ of items. Let $\\mathbf{R}$ of size $|U| \\times |D|$ be the matrix that contains all the ratings that the users have assigned to the items. Also, we assume that we would like to discover $K$ latent features. Our task, then, is to find two matrics $\\mathbf{P}$ (of size $|U| \\times |K|$) and $\\mathbf{Q}$ (of size $|D| \\times |K|$) such that their product apprioximates $\\mathbf{R}$:\n$$\\mathbf{R} \\approx \\mathbf{P} \\times \\mathbf{Q}^T = \\hat{\\mathbf{R}}$$\nIn this way, each row of $\\mathbf{P}$ would represent the strength of the associations between a user and the features. Similarly, each row of $\\mathbf{Q}$ would represent the strength of the associations between an item and the features. To get the prediction of a rating of an item $d_j$ by $u_i$, we can calculate the dot product of their vectors:\n$$\\hat{r}_{ij} = p_i^T q_j = \\sum _{k=1}^{K} p _{ik} q _{kj}$$\nNow, we have to find a way to obtain $\\mathbf{P}$ and $\\mathbf{Q}$. One way to approach this problem is the first intialize the two matrices with some values, calculate how different their product is to $\\mathbf{M}$, and then try to minimize this difference iteratively. Such a method is called gradient descent, aiming at finding a local minimum of the difference.\nThe difference here, usually called the error between the estimated rating and the real rating, can be calculated by the following equation for each user-item pair:\n$$e _{ij}^2 = (r _{ij} - \\hat{r} _{ij})^2 = (r _{ij} - \\sum _{k=1}^K p _{ik} q _{kj})^2$$\nHere we consider the squared error because the estimated rating can be either higher or lower than the real rating.\nTo minimize the error, we have to know in which direction we have to modify the values of $p_{ik}$ and $q_{kj}$. In other words, we need to know the gradient at the current values, and therefore we differentiate the above equation with respect to these two variables separately:\n$$ \\frac{\\partial}{\\partial p _{ik}} e _{ij}^2 = -2(r _{ij} - \\hat{r} _{ij}) (q _{kj}) = -2 e _{ij} q _{kj} $$\n$$ \\frac{\\partial}{\\partial q _{ik}} e _{ij}^2 = -2(r _{ij} - \\hat{r} _{ij}) (p _{ik}) = -2 e _{ij} p _{ik} $$\nHaving obtained the gradient, we can now formulate the update rules for both $p_{ik}$ and $q_{kj}$:\n$$p\u0026rsquo; _{ik} = p _{ik} + \\alpha \\frac{\\partial}{\\partial p _{ik}} e _{ij}^2 = p _{ik} + 2\\alpha e _{ij} q _{kj}$$\n$$q\u0026rsquo; _{kj} = q _{kj} + \\alpha \\frac{\\partial}{\\partial q _{kj}} e _{ij}^2 = q _{kj} + 2\\alpha e _{ij} p _{ik}$$\nHere, $\\alpha$ is a constant whose value determines the rate of approaching the minimum. Usually we will choose a small value for $\\alpha$, say 0.0002. This is because if we make too large a step towards the minimum we may run into the risk of missing the minimum and end up oscillating around the minimum.\nA question might have come to your mind by now: if we find two matrices $\\mathbf{P}$ and $\\mathbf{Q}$ such that $\\mathbf{P} \\times \\mathbf{Q}$ approximates $\\mathbf{R}$, isn\u0026rsquo;t that our predictions of all the unseen ratings will be zeros? In fact, we are not really trying to come up with $\\mathbf{P}$ and $\\mathbf{Q}$ such that we can reproduce $\\mathbf{R}$ exactly. Instead, we will only try to minimise the errors of the observed user-item pairs. In other words, if we let $T$ be a set of tuples, each of which is in the form of $(u_i, d_j, r_{ij})$, such that $T$ contains all the observed user-item pairs together with the associated ratings, we are only trying to minimise every $e_{ij}$ for $(u_i, d_j, r_{ij}) \\in T$. (In other words, $T$ is our set of training data.) As for the rest of the unknowns, we will be able to determine their values once the associations between the users, items and features have been learnt.\nUsing the above update rules, we can then iteratively perform the operation until the error converges to its minimum. We can check the overall error as calculated using the following equation and determine when we should stop the process.\n$$E = \\sum _{(u _i, d _j, r _{ij}) \\in T}{e _{ij}} = \\sum _{(u _i,d _j,r _{ij}) \\in T}{(r _{ij} - \\sum _{k=1}^K p _{ik} q _{kj})^2}$$\nRegularization The above algorithm is a very basic algorithm for factorizing a matrix. There are a lot of methods to make things look more complicated. A common extension to this basic algorithm is to introduce regularization to avoid overfitting. This is done by adding a parameter $\\beta$ and modify the squared error as follows:\n$$e _{ij}^2 = (r _{ij} - \\sum _{k=1}^K p _{ik} q _{kj})^2 + \\frac{\\beta}{2} \\sum _{k=1}^K (||P||^2 + ||Q||^2)$$\nIn other words, the new parameter $\\beta$ is used to control the magnitudes of the user-feature and item-feature vectors such that $P$ and $Q$ would give a good approximation of $R$ without having to contain large numbers. In practice, $\\beta$ is set to some values in the order of 0.02. The new update rules for this squared error can be obtained by a procedure similar to the one described above. The new update rules are as follows:\n$$p\u0026rsquo; _{ik} = p _{ik} + \\alpha \\frac{\\partial}{\\partial p _{ik}} e _{ij}^2 = p _{ik} + \\alpha(2 e _{ij} q _{kj} - \\beta p _{ik} )$$\n$$q\u0026rsquo; _{kj} = q _{kj} + \\alpha \\frac{\\partial}{\\partial q _{kj}} e _{ij}^2 = q _{kj} + \\alpha(2 e _{ij} p _{ik} - \\beta q _{kj} )$$\nAdding Biases When predicting the ratings of users given to items, it is useful to consider how ratings are generated. In the above discussion, we have assumed that ratings are generated based on matching the users preferences on some latent factors and the items\u0026rsquo; characteristics on the latent factors. Actually, it may also be helpful to consider additional factors here.\nFor example, we can assume that when a rating is generated, some biases may also contribute to the ratings. In particular, every user may have his or her own bias, meaning that he or she may tend to rate items higher or lower than the others. In movie ratings, if a user is a serious movie watcher, he or she may tend to give lower ratings, when compared to another user who generally enjoys movies as long as they are not too boring. A similar idea can also be applied to the items being rated.\nHence, in the equal of predicting a rating, we can also add these biases in order to better model how a rating is generated:\nwhere $b$ is the global bias (which can be easily estimated by using the mean of all ratings), $bu_i$ is the bias of user $i$, and $bd_j$ is the bias of item $j$.\nUsing the same steps mentioned above, we can derive the update rules for the user biases and item biases easily:\n$$bu\u0026rsquo; _i = bu _i + \\alpha \\times (e _{ij} - \\beta bu _i)$$\n$$bd\u0026rsquo; _j = bd _j + \\alpha \\times (e _{ij} - \\beta bd _j)$$\nIn practice, the process of factorization will converge faster if biases are included in the model.\nA Simple Implementation in Python Once we have derived the update rules as described above, it actually becomes very straightforward to implement the algorithm. The following is a function that implements the algorithm in Python using the stochastic gradient descent algorithm. Note that this implementation requires the Numpy module.\nimport numpy as np class MF(): def __init__(self, R, K, alpha, beta, iterations): \u0026#34;\u0026#34;\u0026#34; Perform matrix factorization to predict empty entries in a matrix. Arguments - R (ndarray) : user-item rating matrix - K (int) : number of latent dimensions - alpha (float) : learning rate - beta (float) : regularization parameter \u0026#34;\u0026#34;\u0026#34; self.R = R self.num_users, self.num_items = R.shape self.K = K self.alpha = alpha self.beta = beta self.iterations = iterations def train(self): # Initialize user and item latent feature matrice self.P = np.random.normal(scale=1./self.K, size=(self.num_users, self.K)) self.Q = np.random.normal(scale=1./self.K, size=(self.num_items, self.K)) # Initialize the biases self.b_u = np.zeros(self.num_users) self.b_i = np.zeros(self.num_items) self.b = np.mean(self.R[np.where(self.R != 0)]) # Create a list of training samples self.samples = [ (i, j, self.R[i, j]) for i in range(self.num_users) for j in range(self.num_items) if self.R[i, j] \u0026gt; 0 ] # Perform stochastic gradient descent for number of iterations training_process = [] for i in range(self.iterations): np.random.shuffle(self.samples) self.sgd() mse = self.mse() training_process.append((i, mse)) if (i+1) % 10 == 0: print(\u0026#34;Iteration: %d ; error = %.4f\u0026#34; % (i+1, mse)) return training_process def mse(self): \u0026#34;\u0026#34;\u0026#34; A function to compute the total mean square error \u0026#34;\u0026#34;\u0026#34; xs, ys = self.R.nonzero() predicted = self.full_matrix() error = 0 for x, y in zip(xs, ys): error += pow(self.R[x, y] - predicted[x, y], 2) return np.sqrt(error) def sgd(self): \u0026#34;\u0026#34;\u0026#34; Perform stochastic graident descent \u0026#34;\u0026#34;\u0026#34; for i, j, r in self.samples: # Computer prediction and error prediction = self.get_rating(i, j) e = (r - prediction) # Update biases self.b_u[i] += self.alpha * (e - self.beta * self.b_u[i]) self.b_i[j] += self.alpha * (e - self.beta * self.b_i[j]) # Update user and item latent feature matrices self.P[i, :] += self.alpha * (e * self.Q[j, :] - self.beta * self.P[i,:]) self.Q[j, :] += self.alpha * (e * self.P[i, :] - self.beta * self.Q[j,:]) def get_rating(self, i, j): \u0026#34;\u0026#34;\u0026#34; Get the predicted rating of user i and item j \u0026#34;\u0026#34;\u0026#34; prediction = self.b + self.b_u[i] + self.b_i[j] + self.P[i, :].dot(self.Q[j, :].T) return prediction def full_matrix(self): \u0026#34;\u0026#34;\u0026#34; Computer the full matrix using the resultant biases, P and Q \u0026#34;\u0026#34;\u0026#34; return self.b + self.b_u[:,np.newaxis] + self.b_i[np.newaxis:,] + self.P.dot(self.Q.T) We can try to apply it to our example mentioned above and see what we would get. Below is a code snippet in Python for running the example.\nR = np.array([ [5, 3, 0, 1], [4, 0, 0, 1], [1, 1, 0, 5], [1, 0, 0, 4], [0, 1, 5, 4], ]) mf = MF(R, K=2, alpha=0.1, beta=0.01, iterations=20) And the matrix obtained from the above process would look something like this:\n[[ 4.99 3. 3.34 1.01] [ 4. 3.18 2.98 1.01] [ 1.02 0.96 5.54 4.97] [ 1. 0.6 4.78 3.97] [ 1.53 1.05 4.94 4.03]] We can see that for existing ratings we have the approximations very close to the true values, and we also get some \u0026lsquo;predictions\u0026rsquo; of the unknown values. In this simple example, we can easily see that $U1$ and $U2$ have similar taste and they both rated $D1$ and $D2$ high, while the rest of the users preferred $D3$, $D4$ and $D5$. When the number of features ($K$ in the Python code) is 2, the algorithm is able to associate the users and items to two different features, and the predictions also follow these associations. For example, we can see that the predicted rating of $U4$ on $D3$ is 4.78, because $U4$ and $U5$ both rated $D4$ high.\nFurther Information We have discussed the intuitive meaning of the technique of matrix factorization and its use in collaborative filtering. In fact, there are many different extensions to the above technique. An important extension is the requirement that all the elements of the factor matrices $\\mathbf{P}$ and $\\mathbf{Q}$ in the above example) should be non-negative. In this case it is called non-negative matrix factorization (NMF). One advantage of NMF is that it results in intuitive meanings of the resultant matrices. Since no elements are negative, the process of multiplying the resultant matrices to get back the original matrix would not involve subtraction, and can be considered as a process of generating the original data by linear combinations of the latent features.\nSource Code An example can be found at this IPython notebok. It is also available at my Github account in this repository.\nReferences There have been quite a lot of references on matrix factorization. Below are some of the related papers:\nGábor Takács et al (2008). Matrix factorization and neighbor based algorithms for the Netflix prize problemIn: Proceedings of the 2008 ACM Conference on Recommender Systems, Lausanne, Switzerland, October 23 - 25, 267-274. Patrick Ott (2008). Incremental Matrix Factorization for Collaborative Filtering. Science, Technology and Design 01/2008, Anhalt University of Applied Sciences. Daniel D. Lee and H. Sebastian Seung (2001). Algorithms for Non-negative Matrix Factorization. Advances in Neural Information Processing Systems 13: Proceedings of the 2000 Conference. MIT Press. pp. 556–562. Daniel D. Lee and H. Sebastian Seung (1999). Learning the parts of objects by non-negative matrix factorization. Nature, Vol. 401, No. 6755. (21 October 1999), pp. 788-791. ","date":"2017-04-23T00:00:00Z","permalink":"https://albertauyeung.github.io/2017/04/23/python-matrix-factorization.html","title":"🔥 Matrix Factorization: A Simple Tutorial and Implementation in Python"},{"content":"一、 公元一九零五年，是物理學上充滿突破的一年。在這短短的一年內，愛因斯坦 (Albert Einstein) 發表了五篇有關光電物理，分子運動，以及相對論的論文。人們把這一年稱為物理學或愛因斯坦的「奇蹟年」 (Annus Mirabilis)。在工作之餘進行物理學研究的這段時間，愛因斯坦居住在瑞士的伯恩 (Bern) 。伯恩成為愛因斯坦成名的地方，這城市的名字，也就跟這位廿十世紀最偉大科學家的名字連在一起，變得不可分割了。\n二、 瑞士是世人心目中的旅遊勝地，歐洲人享受滑雪運動的好地方。其中日內瓦 (Geneva) 是不少國際機構或組織的總部所在，蘇黎世 (Zürich) 則是歐洲重要的金融中心。可是，作為首都的伯恩，卻是出奇地低調，如果沒有到過瑞士，或是對瑞士不太熟悉的人，大概都說不出這個地方，把其他兩個城市當作瑞士的首都的，也大有人在。\n伯恩是瑞士伯恩州的首府，位於瑞士的德語區，是瑞士繼日內瓦和蘇黎世後的第三大城市。一八四八年，瑞士成立聯邦政府，新憲法將伯恩定為瑞士聯邦首都。相對於日內瓦和蘇黎世，伯恩作為瑞士的行政中心的確是一個比較內向的城市。不過，從遊客的角度來看，伯恩卻絕不比它們遜色。除了城市四周的自然風光，在伯恩市內已被聯合國教科文組織確認為世界遺產的舊城區，可找到十五世紀宏偉的歌德式大教堂，以及一個古老的大鐘樓 (Zytglogge)。舊城區的南面，那勒河 (River Aare) 的對岸，是瑞士幾所重要的博物館的所在地。加上愛因斯坦曾經在這裏居住及發表他學術生涯中最重要的幾份論文，伯恩其實是一個非常值得到訪的地方。\n三、 來伯恩找愛因斯坦的足跡，可先到位於舊城區克拉姆大街 (Kramgasse) 四十九號愛因斯坦的故居。愛因斯坦在蘇黎世聯邦工業大學 (Swiss Federal Institute of Technology Zürich) 畢業後，被伯恩的瑞士專利局聘用為技術員，從事申請專利的鑒定工作。在克拉姆大街的這幢房子的二樓，便是愛因斯坦從一九零三年至一九零五年在伯恩工作時居住的地方。現在，這個房間被保留下來，成為一個小型的愛因斯坦博物館。\n愛因斯坦的故居，跟伯恩這座城市一樣，都很低調。克拉姆大街四十九號的地下，是一間餐廳。房子正門上標著的，便是這間餐廳的名字。「愛因斯坦故居」 (Einstein Haus) 的字卻被夾在中間，跟外牆的顏色更是差不多。若不是順著大街的門號一路找來，相信很容易便會錯過了這個地方。\n故居的房間內，陳列著各種不同的物品，有愛因斯坦讀大學時的成績單、不同的證書的副本、愛因斯坦的照片、他兒子的睡床、還有他穿過的一套西裝。我正在參觀的時候，來了一班中國的留學生，本來已經不是很寬敞的房子裏一下子熱鬧起來。負責賣票的女士見他們一副十分好奇的樣子，走來為他們介紹房間內的物件。\n房間進門右邊展示著愛因斯坦及他的第一任妻子米列娃 (Mileva Marić) 在大學的成績單。米列娃曾經在是蘇黎世聯邦工業大學修讀物理，是愛因斯坦的同學。那位女士告訴我們，把他們的成績單放在一起展覽，是希望大家知道米列娃在數學及物理學的成績也是十分出眾的。的確，從成績單上可見，他們倆無論在數學或物理學的科目上取得很好的成績，幾乎每一科都拿到滿分。可能正因為米列娃也精通數理，不少人提出她可能在愛因斯坦的研究中作出不少貢獻。有些人認為相對論的基本理論是由她提出的，有些人則認為愛因斯坦提出了相對論的中心思想，米列娃則幫助他完成數學上的推論。不過，這些說法無從稽考，也沒有被世人所認真看待。\n成績單上另一個有趣的地方，是在眾多科目中均取得優異成績的愛因斯坦，獨在其中一科「應用物理學」，卻拿了最低的分數。賣票的女士告訴我們，其實愛因斯坦在這一科的成績也是很好，只是他經常曠課，令該課的教授十分不滿，所以給他最低的分數。\n在另一個房間內，放置了一張愛因斯坦在伯恩專利局工作時所使用的書桌。在書桌的四周，牆上展示著不同學者名人對愛因斯坦及其對物理學所作的貢獻的評價，其中也有如霍金 (Stephen Hawking) 等為人所熟知的科學家。在隔壁的房間，放映著一套有關愛因斯坦生平的短片，讓遊人對愛因斯坦的一生有多些了解。\n四、 一九零五年被稱為物理學上的「奇蹟年」，這是由於愛因斯坦在這短短一年內所發表的一共五篇論文，對物理學、天文學、什至整個科學界作出了莫大的貢獻。以下是這五篇論文的題目。\n《關於光的產生和轉化的一個啟發性觀點》(1) 《測定分子大小的新方法》(2) 《根據分子運動論研究靜止液體中懸浮微粒的運動》(3) 《論運動物體的電動力學》(4) 《物體慣性與其所含能量有關嗎？》(5) 這五篇論文中的其中一篇，《測定分子大小的新方法》，是愛因斯坦向蘇黎世大學 (University of Zurich) 提交的博士論文。別以為博士論文一定是長篇大論、頁數過百的文章，愛因斯坦這篇論文只有二十一頁！根據愛因斯坦所述，他第一次提交這篇論文時，大學回覆說篇幅太短，要他作出修改。可是，最終愛因斯坦也只是在原稿中多加了一句句子，便再次提交，並且通過審核，獲頒發博士學位。\n除他的博士論文外，愛因斯坦在這一年內，把其他幾篇論文在一本知名的物理學學術期刊，Annalen der Physik (《物理學年鑑》) ，逐一發表(他的博士論文經修改後在次年也被刊登於這本期刊中)。這些論文一經發表，整個物理學界都驚訝不已。他們的驚訝，不單在於這幾篇論文所涉及的廣泛題目以及獨到和創新的內容，而更在於這些論文竟然是由愛因斯坦──一個並非在大學工作及研究、還未取得博士學位、只在專利局工作的技術員──所發表。愛因斯坦幾乎與跟當時的學者沒有交流，科學上的討論也只限於跟他私人授課的學生以及他的妻子的談話。他的資源相對在大學工作的學者也較為貧乏，可供閱讀的書籍只限於專利局圖書館和他自己的藏書，而沒有大學圖書館那種完備的支援。然而，這幾篇論文所提出的，卻是能改變我們對這個宇宙的理解的重要發現。\n修讀過高中物理學的，一定知道物理學上的「光電效應」 (photoelectric effect) 。上述的其中一篇論文，《關於光的產生和轉化的一個啟發性觀點》，正記載著愛因斯坦對光電效應作出分析的結果。直至十九世紀末期，物理學界一直認為光是一種能量，並以連續不繼的光波 (wave) 的形式存在。可是，當時一些有關光電效應──指金屬在光的照射下發釋放出自由電子──的實驗中，卻出現了很多與波動理論互相矛盾的結果。愛因斯坦在這篇論文中，提出光量子 (photon) 的概念，並利用數學上的推論，精彩地解釋了光電效應的原理。結果，此理論不單為物理學界所接納，更為愛因斯坦帶來一九二一年的諾貝爾物理學獎。\n當然，比起光電效應，愛因斯坦所創立的相對論更為人所熟知。《論運動物體的電動力學》這篇論文正是發表他在狹義相對論的研究成果。其實，愛因斯坦曾經多次因為相對論的研究被提名為諾貝爾得獎者。可是，由於當時評審委員會對理論物理學較為抗拒，加上相對論難以被理解，又缺乏有力的證據，因此愛因斯坦從未因為相對論而獲得諾貝爾獎。愛因斯坦另一廣為人知的發現，是「能量──物質轉換方程式」，即 E=mc2。《物體慣性與其所含能量有關嗎？》這篇論文正是推導出這一方程式的基礎研究。\n五、 愛因斯坦對科學有著重大的貢獻，同時亦是一個傳奇的人物，小小的一所房子，當然不足以讓人們了解他的故事。除了這「愛因斯坦故居」外，伯恩的歷史博物館中，還設有一個資料非常詳盡的愛因斯坦展覽，介紹愛因斯坦的生平。這個展覽，本來是伯恩的歷史博物館在二零零五年時，為慶祝愛因斯坦提出相對論一百週年而籌劃的一個特備展覽，並不是常設的展覽。可是，自展覽設立已來，來參觀的人數不斷增加，並廣受本地人及遊客的歡迎，所以歷史博物館決定保留這個展覽，又乾脆把它命名為「愛因斯坦博物館」。\n「愛因斯坦博物館」所收藏的展品十分豐富，對愛因斯坦生平的描述也十分仔細。愛因斯坦不僅是一個傳奇人物，他的一生，也見證著歷史上很多如兩次世界大戰等重大的事故或變遷，展覽中對這些事件也有著頗為深入的介紹。看過這個展覽後，不單對愛因斯坦的個人生活、學術生涯及人生哲學有了更深刻的了解，對於他的時代和歷史大事的知識也會增進不少。參觀者由愛因斯坦的出生開始，慢慢了解他的童年、他的教育、他在瑞士的生活、因物理學上的研究獲得諾貝爾獎，以及他到美國普林斯頓定居後的生活。當參觀者走到展覽的盡頭，也是愛因斯坦生命的終結。一九五五年四月十八日愛因斯坦於普林斯頓去世。他臨終時說了幾句話，都是用他覺得最親切的德語說的，當時在他身旁的謢士不諳德語，沒能聽懂他最後的說話。\n參考資料 伯恩 Bern - Official Web Site Bern Einstein House Bern History Museum 愛因斯坦 Albert Einstein - Wikipedia Annus Mirabilis Papers - Wikipedia Einstein Year 2005 Einstein Archives Online TIME Magazine 1st July 1946 issue TIME 100: Albert Einstein Michael White and John Gribbin. 1993. Einstein - A Life in Science. London: Simon \u0026amp; Schuster UK Ltd. 愛因斯坦「奇蹟年」論文 (Annus Mirabilis Papers) \u0026ldquo;On a heuristic viewpoint concerning the production and transformation of light.\u0026rdquo; Annalen der Physik, 17:132-148, 1905. \u0026ldquo;A new determination of molecular dimensions. University of Zurich, Ph.D. Dissertation, 30 April 1905. \u0026ldquo;On the motion of small particles suspended in liquids at rest required by the molecular-kinetic theory of heat.\u0026rdquo; Annalen der Physik, 17:549-560, 1905. \u0026ldquo;On the electrodynamics of moving bodies.\u0026rdquo; Annalen der Physik. 17:891-921, 1905. \u0026ldquo;Does the inertia of a body depend upon its energy content?\u0026rdquo; Annalen der Physik, 18:639-641. 1905. 有關愛因斯坦的文學作品 Alan Lightman. 1993. Einstein\u0026rsquo;s Dream. New York: Pantheon Books. Jean-Claude Carrier. 2005. Please, Mr Einstein (Einstein S\u0026rsquo;il Vous Plait). London: Harvill Secker. ","date":"2007-05-04T00:00:00Z","image":"https://albertauyeung.github.io/images/einstein_bern_01.jpg","permalink":"https://albertauyeung.github.io/2007/05/04/einstein-house.html","title":"🧠 愛因斯坦與伯恩"},{"content":"Chateau de chillon 石庸古堡 從洛桑 (Lausanne) 坐火車到蒙特勒 (Montreux)，沿著日內瓦湖 (Lac Léman) 邊望去，可以看到一座碩大的城堡倚立在岸邊。這座古堡，名氣不小。古堡古老莊嚴的建築及四周綺麗的風光自然吸引不少遊人，它還因為英國詩人拜倫 (Lord Byron) 的一首詩而名聲大噪。這座古堡，便是 Château de chillon.\n古堡的中文譯名似乎沒有被統一，售票職員給我的小冊子上寫著「石庸古堡」，可是古堡外的小商店內售賣的書籍卻叫它作「西庸古堡」，我在網路上也看過不少其他的譯名，如「詩庸」、「希隆」等等。若把這些名字跟它的法語名字作比較，也實在沒有太大的分別。無論如何，小冊子的譯名該可算是比較「官方」的吧，暫且用「石庸古堡」這個名字吧。\n到石庸古堡，坐火車到 Veytaux-Chillon 站算是最方便的，下車後在日內瓦湖邊步行 10 分鐘左右便可到達。可是，這畢竟是一個小車站，來往大城市的列車不停於此。另一個方法是坐火車到蒙特勒，從火車站坐巴士來到城堡的門前下車。\n雖說拜倫的詩為這座古堡添了不少名氣，可是它本身對遊人來說也是十分吸引的。一般城堡，多建在山丘高地上，居高臨下，以便防禦外來的襲擊。但是，石庸古堡卻建在湖邊的一個小島上，流過岸邊和島石間的日內瓦湖水便成了它自然的謢城河。日內瓦湖平靜如鏡，在古堡高處依稀可見湖的彼岸，四周是高聳入雲的雪山，映在湖面上，更覺宏偉。\n石庸古堡的歷史，最早的記載始自公元一零零五年。可是，古堡的圍牆可追溯至中世紀時代。古堡所在的小島，更因在城堡入口附近所發掘到的墳墓，被證實在青銅時代已經有人類聚居。在附近發掘到的其他文物，也證明羅馬人曾經佔領此地。石庸古堡位處瑞士通往意大利的山道，自古以來即是軍事要地。古堡本來為錫永主教(Bishops of Sion) 所有，自十二世紀起則成為薩伏依公爵 (Dukes of Savoy) 的財產，直到一五三六年為瑞士人所攻佔。瑞士伯恩 (Bern) 的軍隊佔領城堡後，曾經用作大法官的倉庫、軍火庫、府第，以及監獄。古堡的塔樓和城垛便是在這個時期建造的。至一七九八年，沃州 (Canton of Vaud) 取得獨立，石庸古堡從此成為沃州的財產。一八八七年，瑞士成立了「石庸古堡修復協會」 (Association du Château de Chillon)，專責古堡的修復及圍謢工作。從它的歷史，可見石庸古堡曾經多次易手，而不同的主人也曾經進行多次修復及增建的工作。正因如此，石庸古堡的不同部份的建築雖然色調一致，但風格卻是有所不同，這也成為了古堡的特色之一。\n石庸古堡成為各國遊客趨之若鶩的瑞士景點，乃是因為十九世紀英國浪漫詩人拜倫所寫的一首題為〈石庸的囚徒〉 (The Prisoner of Chillon) 的詩。一八一六年，拜倫跟居住在瑞士的友人、英國詩人雪萊 (Shelley) 一同遊覽日內瓦湖，並到石庸古堡參觀。拜倫在參觀期間，得知日內瓦一位民族英雄波尼伐 (François Bonivard) 因支持日內瓦獨立而被囚禁於古堡地下囚牢的事蹟，有感而發寫下一首題為 \u0026ldquo;Sonnet on Chillon\u0026rdquo; 的詩歌。其後在洛桑暫居的拜倫，基於波尼伐的事蹟，繼續完成了長詩〈石庸的囚徒〉，並在回到英國後出版。\n拜倫所到過的囚禁波尼伐的牢房，現在應該是古堡最受人注目的地方。地下室的入口，在近城堡入口的一號院的後方。走下梯階，穿過地下拱室，便是古堡的囚牢。當年主張日內瓦獨立的波尼伐，便是被鐵鏈鎖在囚牢中的第五根柱子上。既然是瑞士第一古堡的地下囚房，又是拜倫詩歌中描述的地方，柱子上難免被刻上來自不同國家的遊客的「到此一遊」字句。畢竟，拜倫自己也在第三根柱子上刻下自己的名字。不過，貴人名士的塗鴉終究是有點不同，現在拜倫的簽名已經被玻璃保護著了。\n拜倫的詩，講述波尼伐及他的兩位弟弟被關押於此。他們被鎖在不同的柱子上，可以看見對方，卻觸摸不到。波尼伐的兩位弟弟沒能支撐到最後，都死在牢獄之中，只剩下波尼伐一人。拜倫的詩，實在是一個可歌可泣的悲慘故事。可是，「文人多大話」這句話可是沒有說錯。詩中的主角波尼伐，是日內瓦聖維克多修道院 (St. Victor) 的院長，十六世紀初因主張及推動日內瓦獨立，及對抗薩伏依公爵，而被囚禁於古堡之中。被囚禁的四年後，瑞士人成功攻佔石庸城堡，波尼伐終被釋放出來。歷史上，波尼伐的兩位弟弟卻是不存在的。無論如何，拜倫優美的文字和富感染力的描述，確實為石庸古堡添加了不少傳奇色彩。\n在城堡內，幾乎所有的房間及設施都是開放給遊人參觀的。可是在近城門的一號院的一角，在十六世紀時曾經用作馬棚的地方，已被改建成新式房屋，小冊子上寫著現在是公寓。竟然有人住在一個對外開放給遊人參觀的歷史古蹟，實在有點奇怪。除了曾經囚禁波尼伐的監獄外，其他地方如堡主大廳、宴會廳、紋章大廳、公爵的卧室等等，都是很值得參觀的地方。遊人更可走上城堡的主塔，俯瞰城堡的宏偉建築或眺望四周壯麗的景色。\n古堡及日內瓦湖畔，春夏秋冬各有不同的景致，雖然多數遊人不會有機會親身體會，在古堡外的小商店內出售的明信片及書籍，卻也可讓遊人感受一下此地四季不 同的氣氛。\n參考資料 Château de chillon 官方網站 Château de chillon - Wikipedia Château de chillon on Google Maps George Gordon Byron - Wikipedia François Bonivard - Wikipedia The Prisoner of Chillon (Full Text) - Wikisource 延伸閱讀 余秋雨，〈希隆的囚徒〉，《行者無疆》，時報文化，台北，二零零一年。 欒珊瑚，《漫步蕾夢湖》，元尊文化，台北，一九九八年。 ","date":"2007-04-25T00:00:00Z","image":"https://albertauyeung.github.io/images/chillon00.jpg","permalink":"https://albertauyeung.github.io/2007/04/25/chateau-chillon.html","title":"🏰 Château de Chillon 瑞士石庸古堡遊記"}]