<!DOCTYPE html>
<html lang="en" class="h-full">
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-30554548-1"></script>
    <script>window.dataLayer = window.dataLayer || [];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());gtag('config', 'UA-30554548-1');</script>

    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="turbo-cache-control" content="no-cache">

    <!-- Primary Meta Tags -->
    <title>BERT - Tokenization and Encoding</title>
    <meta name="title" content="BERT - Tokenization and Encoding">
    <meta name="description" content="To use a pre-trained BERT model, we need to convert the input data into an appropriate format so that each sentence can be sent to the pre-trained..." />

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://albertauyeung.github.io/2020/06/19/bert-tokenization.html/">
    <meta property="og:title" content="BERT - Tokenization and Encoding">
    <meta property="og:description" content="To use a pre-trained BERT model, we need to convert the input data into an appropriate format so that each sentence can be sent to the pre-trained...">

    <!-- Twitter -->
    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:url" content="https://albertauyeung.github.io/2020/06/19/bert-tokenization.html/">
    <meta property="twitter:title" content="BERT - Tokenization and Encoding">
    <meta property="twitter:description" content="To use a pre-trained BERT model, we need to convert the input data into an appropriate format so that each sentence can be sent to the pre-trained...">
    
    <script>(function () { var el = document.documentElement, m = localStorage.getItem("doc_theme"), wm = window.matchMedia; if (m === "dark" || (!m && wm && wm("(prefers-color-scheme: dark)").matches)) { el.classList.add("dark") } else { el.classList.remove("dark") } })();</script>

    <link href="/resources/css/retype.css?v=1.10.688750935963" rel="stylesheet" data-turbo-track="reload" />

    <script type="text/javascript" src="/resources/js/config.js?v=1.10.688750935963" defer data-turbo-track="reload"></script>
    <script type="text/javascript" src="/resources/js/retype.js?v=1.10" defer data-turbo-track="reload"></script>
    <script id="prism-js" type="text/javascript" src="/resources/js/prism.js?v=1.10.688750935963" defer></script>
<style>
.docs-markdown h1 { font-size: 1.75rem; }
.docs-markdown h2 { font-size: 1.45rem; }
.text-base { font-size: 0.95rem; }
</style>
</head>
    <body>
        <div id="docs-app" class="relative text-base antialiased text-gray-700 bg-white font-body dark:bg-dark-850 dark:text-dark-300">
    <div class="absolute bottom-0 left-0 bg-gray-100 dark:bg-dark-800" style="top: 5rem; right: 50%"></div>

    <header id="docs-site-header" class="sticky top-0 z-30 flex w-full h-16 bg-white border-b border-gray-200 md:h-20 dark:bg-dark-850 dark:border-dark-650">
    <div class="container relative flex items-center justify-between flex-grow pr-6 md:justify-start">
        <!-- Mobile menu button skeleton -->
        <button v-cloak class="skeleton docs-mobile-menu-button flex items-center justify-center flex-shrink-0 overflow-hidden dark:text-white focus:outline-none rounded-full w-10 h-10 ml-3.5 md:hidden"><svg xmlns="http://www.w3.org/2000/svg" class="mb-px flex-shrink-0" width="24" height="24" viewBox="0 0 24 24" role="presentation" style="margin-bottom: 0px;"><g fill="currentColor"><path d="M2 4h20v2H2zM2 11h20v2H2zM2 18h20v2H2z"></path></g></svg></button>
        <div v-cloak id="docs-sidebar-toggle"></div>

        <!-- Logo -->
        <div class="flex items-center justify-between h-full py-2 md:w-75">
            <div class="flex items-center px-2 md:px-6">
                <a id="docs-site-logo" href="/" class="flex items-center leading-snug text-xl">
                    <span class="dark:text-white font-semibold line-clamp-1 md:line-clamp-2">Albert Au Yeung</span>
                </a>
            </div>

            <span class="hidden h-8 border-r md:inline-block dark:border-dark-650"></span>
        </div>

        <div class="flex justify-between md:flex-grow">
            <!-- Top Nav -->
            <nav class="hidden md:flex">
    <ul class="flex flex-col mb-4 md:pl-16 md:mb-0 md:flex-row md:items-center">
        <li class="md:mr-6">
            <a class="inline-flex items-center py-2 md:mb-0 text-sm text-gray-400 whitespace-nowrap transition-colors duration-200 ease-linear md:text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200" href="https://www.linkedin.com/in/albert-au-yeung/">
                <svg xmlns="http://www.w3.org/2000/svg" class="mb-px mr-1" width="18" height="18" viewBox="0 0 24 24" role="presentation">
                    <g fill="currentColor">
                        <path d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-3.368-4-3.113-4 0v5.604h-3v-11h3v1.765c1.396-2.586 7-2.777 7 2.476v6.759z"/>
                    </g>
                </svg>
                <span>LinkedIn</span>
            </a>
        </li>
        <li class="md:mr-6">
            <a class="inline-flex items-center py-2 md:mb-0 text-sm text-gray-400 whitespace-nowrap transition-colors duration-200 ease-linear md:text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200" href="https://www.github.com/albertauyeung">
                <svg xmlns="http://www.w3.org/2000/svg" class="mb-px mr-1" width="18" height="18" viewBox="0 0 24 24" role="presentation">
                    <g fill="currentColor">
                        <path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/>
                    </g>
                </svg>
                <span>Github</span>
            </a>
        </li>
        <li class="md:mr-6">
            <a class="inline-flex items-center py-2 md:mb-0 text-sm text-gray-400 whitespace-nowrap transition-colors duration-200 ease-linear md:text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200" href="https://twitter.com/albertauyeung">
                <svg xmlns="http://www.w3.org/2000/svg" class="mb-px mr-1" width="18" height="18" viewBox="0 0 24 24" role="presentation">
                    <g fill="currentColor">
                        <g fill="currentColor"><path d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-.139 9.237c.209 4.617-3.234 9.765-9.33 9.765-1.854 0-3.579-.543-5.032-1.475 1.742.205 3.48-.278 4.86-1.359-1.437-.027-2.649-.976-3.066-2.28.515.098 1.021.069 1.482-.056-1.579-.317-2.668-1.739-2.633-3.26.442.246.949.394 1.486.411-1.461-.977-1.875-2.907-1.016-4.383 1.619 1.986 4.038 3.293 6.766 3.43-.479-2.053 1.08-4.03 3.199-4.03.943 0 1.797.398 2.395 1.037.748-.147 1.451-.42 2.086-.796-.246.767-.766 1.41-1.443 1.816.664-.08 1.297-.256 1.885-.517-.439.656-.996 1.234-1.639 1.697z"></path></g>
                    </g>
                </svg>
                <span>Twitter</span>
            </a>
        </li>
    </ul>
</nav>

            <!-- Header Right Skeleton -->
            <div v-cloak class="flex justify-end flex-grow skeleton">

                <!-- Search input mock -->
                <div class="relative hidden w-40 lg:block lg:max-w-sm lg:ml-auto">
                    <div class="absolute flex items-center justify-center h-full pl-3 dark:text-dark-300">
                        <svg xmlns="http://www.w3.org/2000/svg" class="icon-base" width="16" height="16" viewBox="0 0 24 24" aria-labelledby="icon" role="presentation"  style="margin-bottom: 1px;"><g fill="currentColor" ><path d="M21.71 20.29l-3.68-3.68A8.963 8.963 0 0020 11c0-4.96-4.04-9-9-9s-9 4.04-9 9 4.04 9 9 9c2.12 0 4.07-.74 5.61-1.97l3.68 3.68c.2.19.45.29.71.29s.51-.1.71-.29c.39-.39.39-1.03 0-1.42zM4 11c0-3.86 3.14-7 7-7s7 3.14 7 7c0 1.92-.78 3.66-2.04 4.93-.01.01-.02.01-.02.01-.01.01-.01.01-.01.02A6.98 6.98 0 0111 18c-3.86 0-7-3.14-7-7z" ></path></g></svg>
                    </div>

                    <input class="w-full h-10 transition-colors duration-200 ease-in bg-gray-200 border border-transparent rounded md:text-sm hover:bg-white hover:border-gray-300 focus:outline-none focus:bg-white focus:border-gray-500 dark:bg-dark-600 dark:border-dark-600 placeholder-gray-400 dark:placeholder-dark-400"
                    style="padding: 0.625rem 0.75rem 0.625rem 2rem" type="text" placeholder="Search docs" />
                </div>

                <!-- Mobile search button mock -->
                <div class="flex items-center justify-center w-10 h-10 lg:hidden">
                    <svg xmlns="http://www.w3.org/2000/svg" class="flex-shrink-0 icon-base" width="20" height="20" viewBox="0 0 24 24" aria-labelledby="icon" role="presentation"  style="margin-bottom: 0px;"><g fill="currentColor" ><path d="M21.71 20.29l-3.68-3.68A8.963 8.963 0 0020 11c0-4.96-4.04-9-9-9s-9 4.04-9 9 4.04 9 9 9c2.12 0 4.07-.74 5.61-1.97l3.68 3.68c.2.19.45.29.71.29s.51-.1.71-.29c.39-.39.39-1.03 0-1.42zM4 11c0-3.86 3.14-7 7-7s7 3.14 7 7c0 1.92-.78 3.66-2.04 4.93-.01.01-.02.01-.02.01-.01.01-.01.01-.01.02A6.98 6.98 0 0111 18c-3.86 0-7-3.14-7-7z" ></path></g></svg>
                </div>

                <!-- Dark mode switch placehokder -->
                <div class="w-10 h-10 lg:ml-2"></div>

                <!-- History button mock -->
                <div class="flex items-center justify-center w-10 h-10" style="margin-right: -0.625rem;">
                    <svg xmlns="http://www.w3.org/2000/svg" class="flex-shrink-0 icon-base" width="22" height="22" viewBox="0 0 24 24" aria-labelledby="icon" role="presentation"  style="margin-bottom: 0px;"><g fill="currentColor" ><g ><path d="M12.01 6.01c-.55 0-1 .45-1 1V12a1 1 0 00.4.8l3 2.22a.985.985 0 001.39-.2.996.996 0 00-.21-1.4l-2.6-1.92V7.01c.02-.55-.43-1-.98-1z"></path><path d="M12.01 1.91c-5.33 0-9.69 4.16-10.05 9.4l-.29-.26a.997.997 0 10-1.34 1.48l1.97 1.79c.19.17.43.26.67.26s.48-.09.67-.26l1.97-1.79a.997.997 0 10-1.34-1.48l-.31.28c.34-4.14 3.82-7.41 8.05-7.41 4.46 0 8.08 3.63 8.08 8.09s-3.63 8.08-8.08 8.08c-2.18 0-4.22-.85-5.75-2.4a.996.996 0 10-1.42 1.4 10.02 10.02 0 007.17 2.99c5.56 0 10.08-4.52 10.08-10.08.01-5.56-4.52-10.09-10.08-10.09z"></path></g></g></svg>
                </div>
            </div>

            <div v-cloak class="flex items-center justify-end flex-grow">
                <div id="docs-mobile-search-button"></div>
                <doc-search-desktop></doc-search-desktop>

                <doc-theme-switch class="lg:ml-2"></doc-theme-switch>
                <doc-history></doc-history>
            </div>
        </div>
    </div>
</header>


    <div class="container relative flex bg-white">
        <!-- Sidebar Skeleton -->
<div v-cloak class="fixed flex flex-col flex-shrink-0 duration-300 ease-in-out bg-gray-100 border-gray-200 sidebar top-20 w-75 border-r h-screen md:sticky transition-transform skeleton dark:bg-dark-800 dark:border-dark-650">

    <!-- Render this div, if config.showSidebarFilter is `true` -->
    <div class="flex items-center h-16 px-6">
        <input class="w-full h-8 px-3 py-2 transition-colors duration-200 ease-linear bg-white border border-gray-200 rounded shadow-none text-sm focus:outline-none focus:border-gray-600 dark:bg-dark-600 dark:border-dark-600" type="text" placeholder="Filter" />
    </div>

    <div class="pl-6 mb-4 mt-1">
        <div class="w-32 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
        <div class="w-48 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
        <div class="w-40 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
        <div class="w-32 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
        <div class="w-48 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
        <div class="w-40 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
    </div>

    <div class="flex-shrink-0 mt-auto bg-transparent dark:border-dark-650">
        <a
    class="flex items-center justify-center flex-nowrap h-16 text-gray-400 dark:text-dark-400 hover:text-gray-700 dark:hover:text-dark-300 transition-colors duration-150 ease-in"
    target="_blank"
    href="https://retype.com/"
    rel="noopener"
>
    <span class="text-xs whitespace-nowrap">Powered by</span>
    <svg xmlns="http://www.w3.org/2000/svg" class="ml-2" fill="currentColor" width="96" height="20" overflow="visible"><path d="M0 0v20h13.59V0H0zm11.15 17.54H2.44V2.46h8.71v15.08zM15.8 20h2.44V4.67L15.8 2.22zM20.45 6.89V20h2.44V9.34z"/><g><path d="M40.16 8.44c0 1.49-.59 2.45-1.75 2.88l2.34 3.32h-2.53l-2.04-2.96h-1.43v2.96h-2.06V5.36h3.5c1.43 0 2.46.24 3.07.73s.9 1.27.9 2.35zm-2.48 1.1c.26-.23.38-.59.38-1.09 0-.5-.13-.84-.4-1.03s-.73-.28-1.39-.28h-1.54v2.75h1.5c.72 0 1.2-.12 1.45-.35zM51.56 5.36V7.2h-4.59v1.91h4.13v1.76h-4.13v1.92h4.74v1.83h-6.79V5.36h6.64zM60.09 7.15v7.48h-2.06V7.15h-2.61V5.36h7.28v1.79h-2.61zM70.81 14.64h-2.06v-3.66l-3.19-5.61h2.23l1.99 3.45 1.99-3.45H74l-3.19 5.61v3.66zM83.99 6.19c.65.55.97 1.4.97 2.55s-.33 1.98-1 2.51-1.68.8-3.04.8h-1.23v2.59h-2.06V5.36h3.26c1.42 0 2.45.28 3.1.83zm-1.51 3.65c.25-.28.37-.69.37-1.22s-.16-.92-.48-1.14c-.32-.23-.82-.34-1.5-.34H79.7v3.12h1.38c.68 0 1.15-.14 1.4-.42zM95.85 5.36V7.2h-4.59v1.91h4.13v1.76h-4.13v1.92H96v1.83h-6.79V5.36h6.64z"/></g></svg>
</a>

    </div>
</div>

<!-- Sidebar component -->
<doc-sidebar v-cloak>
    <template #sidebar-footer>
        <div
            class="flex-shrink-0 mt-auto border-t md:bg-transparent md:border-none dark:border-dark-650"
        >

            <div class="py-3 px-6 md:hidden border-b dark:border-dark-650">
                <nav>
                    <ul class="flex flex-wrap justify-center items-center">
                        <li class="mr-6">
                            <a class="block py-1 text-sm whitespace-nowrap transition-colors duration-200 ease-linear text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200" href="https://www.linkedin.com/in/albert-au-yeung/">LinkedIn</a>
                        </li>
                        <li class="mr-6">
                            <a class="block py-1 text-sm whitespace-nowrap transition-colors duration-200 ease-linear text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200" href="https://www.github.com/albertauyeung">Github</a>
                        </li>
                        <li class="mr-6">
                            <a class="block py-1 text-sm whitespace-nowrap transition-colors duration-200 ease-linear text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200" href="https://twitter.com/albertauyeung">Twitter</a>
                        </li>
                    </ul>
                </nav>
            </div>

            <a
    class="flex items-center justify-center flex-nowrap h-16 text-gray-400 dark:text-dark-400 hover:text-gray-700 dark:hover:text-dark-300 transition-colors duration-150 ease-in"
    target="_blank"
    href="https://retype.com/"
    rel="noopener"
>
    <span class="text-xs whitespace-nowrap">Powered by</span>
    <svg xmlns="http://www.w3.org/2000/svg" class="ml-2" fill="currentColor" width="96" height="20" overflow="visible"><path d="M0 0v20h13.59V0H0zm11.15 17.54H2.44V2.46h8.71v15.08zM15.8 20h2.44V4.67L15.8 2.22zM20.45 6.89V20h2.44V9.34z"/><g><path d="M40.16 8.44c0 1.49-.59 2.45-1.75 2.88l2.34 3.32h-2.53l-2.04-2.96h-1.43v2.96h-2.06V5.36h3.5c1.43 0 2.46.24 3.07.73s.9 1.27.9 2.35zm-2.48 1.1c.26-.23.38-.59.38-1.09 0-.5-.13-.84-.4-1.03s-.73-.28-1.39-.28h-1.54v2.75h1.5c.72 0 1.2-.12 1.45-.35zM51.56 5.36V7.2h-4.59v1.91h4.13v1.76h-4.13v1.92h4.74v1.83h-6.79V5.36h6.64zM60.09 7.15v7.48h-2.06V7.15h-2.61V5.36h7.28v1.79h-2.61zM70.81 14.64h-2.06v-3.66l-3.19-5.61h2.23l1.99 3.45 1.99-3.45H74l-3.19 5.61v3.66zM83.99 6.19c.65.55.97 1.4.97 2.55s-.33 1.98-1 2.51-1.68.8-3.04.8h-1.23v2.59h-2.06V5.36h3.26c1.42 0 2.45.28 3.1.83zm-1.51 3.65c.25-.28.37-.69.37-1.22s-.16-.92-.48-1.14c-.32-.23-.82-.34-1.5-.34H79.7v3.12h1.38c.68 0 1.15-.14 1.4-.42zM95.85 5.36V7.2h-4.59v1.91h4.13v1.76h-4.13v1.92H96v1.83h-6.79V5.36h6.64z"/></g></svg>
</a>

        </div>
    </template>
</doc-sidebar>


        <div class="flex-grow min-w-0 dark:bg-dark-850">
            <!-- Render "toolbar" template here on api pages --><!-- Render page content -->
            <div class="flex">
    <div class="flex-grow min-w-0 px-6 md:px-16">
        <main class="relative pt-6 pb-16">
            <div class="docs-markdown" id="docs-content">
                <!-- Rendered if sidebar right is enabled -->
                <div id="docs-sidebar-right-toggle"></div>
               
                <!-- Page content  -->
<doc-anchor-target id="bert---tokenization-and-encoding" class="break-words">
    <h1>
        <doc-anchor-trigger class="header-anchor-trigger" to="#bert---tokenization-and-encoding">#</doc-anchor-trigger>
        <span>BERT - Tokenization and Encoding</span>
    </h1>
</doc-anchor-target>
<div class="-mt-3 mb-12 flex flex-wrap text-sm text-gray-400 dark:text-dark-350">
    <div class="flex items-center flex-shrink-0">Published&nbsp;<span>2020-06-19</span></div>
</div>

<p>To use a pre-trained BERT model, we need to convert the input data into an appropriate format so that each sentence can be sent to the pre-trained model to obtain the corresponding embedding. This article introduces how this can be done using modules and functions available in Hugging Face&#x27;s <code v-pre>transformers</code> package (<a href="https://huggingface.co/transformers/index.html">https://huggingface.co/transformers/index.html</a>).</p>
<doc-anchor-target id="input-representation-in-bert">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#input-representation-in-bert">#</doc-anchor-trigger>
        <span>Input Representation in BERT</span>
    </h2>
</doc-anchor-target>
<p>Let&#x27;s first try to understand how an input sentence should be represented in BERT. BERT embeddings are trained with two training tasks:</p>
<ol>
<li><strong>Classification Task</strong>: to determine which category the input sentence should fall into</li>
<li><strong>Next Sentence Prediction Task</strong>: to determine if the second sentence naturally follows the first sentence.</li>
</ol>
<doc-anchor-target id="the-cls-and-sep-tokens">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#the-cls-and-sep-tokens">#</doc-anchor-trigger>
        <span>The <code v-pre>[CLS]</code> and <code v-pre>[SEP]</code> Tokens</span>
    </h3>
</doc-anchor-target>
<p>For the classification task, a <strong>single</strong> vector representing the whole input sentence is needed to be fed to a classifier. In BERT, the decision is that the hidden state of the <strong>first token</strong> is taken to represent the whole sentence. To achieve this, an additional token has to be added manually to the input sentence. In the original implementation, the token <code v-pre>[CLS]</code> is chosen for this purpose.</p>
<p>In the &quot;next sentence prediction&quot; task, we need a way to inform the model where does the <strong>first sentence end</strong>, and where does the <strong>second sentence begin</strong>. Hence, another artificial token, <code v-pre>[SEP]</code>, is introduced. If we are trying to train a classifier, each input sample will contain only one sentence (or a single text input). In that case, the <code v-pre>[SEP]</code> token will be added to the end of the input text.</p>
<p>In summary, to preprocess the input text data, the first thing we will have to do is to add the <code v-pre>[CLS]</code> token at the beginning, and the <code v-pre>[SEP]</code> token at the end of each input text.</p>
<doc-anchor-target id="padding-token-pad">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#padding-token-pad">#</doc-anchor-trigger>
        <span>Padding Token <code v-pre>[PAD]</code></span>
    </h3>
</doc-anchor-target>
<p>The BERT model receives a fixed length of sentence as input. Usually the maximum length of a sentence depends on the data we are working on. For sentences that are shorter than this maximum length, we will have to add paddings (empty tokens) to the sentences to make up the length. In the original implementation, the token <code v-pre>[PAD]</code> is used to represent paddings to the sentence.</p>
<doc-anchor-target id="converting-tokens-to-ids">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#converting-tokens-to-ids">#</doc-anchor-trigger>
        <span>Converting Tokens to IDs</span>
    </h3>
</doc-anchor-target>
<p>When the BERT model was trained, each token was given a <strong>unique ID</strong>. Hence, when we want to use a pre-trained BERT model, we will first need to convert each token in the input sentence into its corresponding unique IDs.</p>
<p>There is an important point to note when we use a pre-trained model. Since the model is pre-trained on a certain corpus, the <strong>vocabulary</strong> was also fixed. In other words, when we apply a pre-trained model to some other data, it is possible that some tokens in the new data might not appear in the fixed vocabulary of the pre-trained model. This is commonly known as the <strong>out-of-vocabulary (OOV)</strong> problem.</p>
<p>For tokens not appearing in the original vocabulary, it is designed that they should be replaced with a special token <code v-pre>[UNK]</code>, which stands for <strong>unknown</strong> token.</p>
<p>However, converting all unseen tokens into <code v-pre>[UNK]</code> will take away a lot of information from the input data. Hence, BERT makes use of a <strong>WordPiece</strong> algorithm that breaks a word into several <em>subwords</em>, such that commonly seen subwords can also be represented by the model.</p>
<p>For example, the word <code v-pre>characteristically</code> does not appear in the original vocabulary. Nevertheless, when we use the BERT tokenizer to tokenize a sentence containing this word, we get something as shown below:</p>
<div class="codeblock-wrapper"><doc-codeblock>
<pre class="language-python"><code v-pre class="language-python">&gt;&gt;&gt; from transformers import BertTokenizer
&gt;&gt;&gt; tz = BertTokenizer.from_pretrained(&quot;bert-base-cased&quot;)
&gt;&gt;&gt; tz.convert_tokens_to_ids([&quot;characteristically&quot;])
[100]

&gt;&gt;&gt; sent = &quot;He remains characteristically confident and optimistic.&quot;
&gt;&gt;&gt; tz.tokenize(sent)
['He',
 'remains',
 'characteristic',
 '##ally',
 'confident',
 'and',
 'optimistic',
 '.']

&gt;&gt;&gt; tz.convert_tokens_to_ids(tz.tokenize(sent))
[1124, 2606, 7987, 2716, 9588, 1105, 24876, 119]</code></pre>
</doc-codeblock></div>
<p>We can see that the word <code v-pre>characteristically</code> will be converted to the ID <code v-pre>100</code>, which is the ID of the token <code v-pre>[UNK]</code>, if we do not apply the tokenization function of the BERT model.</p>
<p>The BERT tokenization function, on the other hand, will first breaks the word into two subwoards, namely <code v-pre>characteristic</code> and <code v-pre>##ally</code>, where the first token is a more commonly-seen word (prefix) in a corpus, and the second token is prefixed by two hashes <code v-pre>##</code> to indicate that it is a suffix following some other subwords.</p>
<p>After this tokenization step, all tokens can be converted into their corresponding IDs.</p>
<doc-anchor-target id="summary">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#summary">#</doc-anchor-trigger>
        <span>Summary</span>
    </h3>
</doc-anchor-target>
<p>In summary, an input sentence for a <strong>classification task</strong> will go through the following steps before being fed into the BERT model.</p>
<ol>
<li>Tokenization: breaking down of the sentence into tokens</li>
<li>Adding the <code v-pre>[CLS]</code> token at the beginning of the sentence</li>
<li>Adding the <code v-pre>[SEP]</code> token at the end of the sentence</li>
<li>Padding the sentence with <code v-pre>[PAD]</code> tokens so that the total length equals to the maximum length</li>
<li>Converting each token into their corresponding IDs in the model</li>
</ol>
<p>An example of preparing a sentence for input to the BERT model is shown below. For simplicity, we assume the maximum length is 10 in the example below (while in the original model it is set to be 512).</p>
<div class="codeblock-wrapper"><doc-codeblock>
<pre class="language-markdown"><code v-pre class="language-markdown"># Original Sentence
Let's learn deep learning!

# Tokenized Sentence
['Let', &quot;'&quot;, 's', 'learn', 'deep', 'learning', '!']

# Adding [CLS] and [SEP] Tokens
['[CLS]', 'Let', &quot;'&quot;, 's', 'learn', 'deep', 'learning', '!', '[SEP]']

# Padding
['[CLS]', 'Let', &quot;'&quot;, 's', 'learn', 'deep', 'learning', '!', '[SEP]', '[PAD]']

# Converting to IDs
[101, 2421, 112, 188, 3858, 1996, 3776, 106, 102, 0]</code></pre>
</doc-codeblock></div>
<doc-anchor-target id="tokenization-using-the-transformers-package">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#tokenization-using-the-transformers-package">#</doc-anchor-trigger>
        <span>Tokenization using the transformers Package</span>
    </h2>
</doc-anchor-target>
<p>While there are quite a number of steps to transform an input sentence into the appropriate representation, we can use the functions provided by the <code v-pre>transformers</code> package to help us perform the tokenization and transformation easily. In particular, we can use the function <code v-pre>encode_plus</code>, which does the following in one go:</p>
<ol>
<li>Tokenize the input sentence</li>
<li>Add the <code v-pre>[CLS]</code> and <code v-pre>[SEP]</code> tokens.</li>
<li>Pad or truncate the sentence to the maximum length allowed</li>
<li>Encode the tokens into their corresponding IDs
Pad or truncate all sentences to the same length.</li>
<li>Create the attention masks which explicitly differentiate real tokens from <code v-pre>[PAD]</code> tokens</li>
</ol>
<p>The following codes shows how this can be done.</p>
<div class="codeblock-wrapper"><doc-codeblock>
<pre class="language-python"><code v-pre class="language-python"># Import tokenizer from transformers package
from transformers import BertTokenizer

# Load the tokenizer of the &quot;bert-base-cased&quot; pretrained model
# See https://huggingface.co/transformers/pretrained_models.html for other models
tz = BertTokenizer.from_pretrained(&quot;bert-base-cased&quot;)

# The senetence to be encoded
sent = &quot;Let's learn deep learning!&quot;

# Encode the sentence
encoded = tz.encode_plus(
    text=sent,  # the sentence to be encoded
    add_special_tokens=True,  # Add [CLS] and [SEP]
    max_length = 64,  # maximum length of a sentence
    pad_to_max_length=True,  # Add [PAD]s
    return_attention_mask = True,  # Generate the attention mask
    return_tensors = 'pt',  # ask the function to return PyTorch tensors
)

# Get the input IDs and attention mask in tensor format
input_ids = encoded['input_ids']
attn_mask = encoded['attention_mask']</code></pre>
</doc-codeblock></div>
<p>After executing the codes above, we will have the following content for the <code v-pre>input_ids</code> and <code v-pre>attn_mask</code> variables:</p>
<div class="codeblock-wrapper"><doc-codeblock>
<pre class="language-python"><code v-pre class="language-python">&gt;&gt;&gt; input-ids
tensor([[ 101, 2421,  112,  188, 3858, 1996, 3776,  106,  102,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0]])
&gt;&gt;&gt;
&gt;&gt;&gt; attn_mask
tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])</code></pre>
</doc-codeblock></div>
<p>The <strong>&quot;attention mask&quot;</strong> tells the model which tokens should be attended to and which (the <code v-pre>[PAD]</code> tokens) should not (see the <a href="https://huggingface.co/transformers/glossary.html#attention-mask">documentation</a> for more detail). It will be needed when we feed the input into the BERT model.</p>
<doc-anchor-target id="reference">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#reference">#</doc-anchor-trigger>
        <span>Reference</span>
    </h2>
</doc-anchor-target>
<ul>
<li>Devlin et al. 2018. <a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></li>
<li>BERT - transformers documentation: <a href="https://huggingface.co/transformers/model_doc/bert.html">https://huggingface.co/transformers/model_doc/bert.html</a></li>
</ul>


<div class="flex items-center mt-6 mb-6">
    <span>
        <a href="/tags/">
            <svg class="inline-block -mb-px mr-1.5 text-blue-500 dark:text-blue-400" xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" overflow="visible" fill="currentColor"><g><path d="M21.3 9.88l-8.59-8.59C12.52 1.11 12.27 1 12 1H2c-.55 0-1 .45-1 1v10c0 .27.11.52.29.71l8.59 8.58c.57.57 1.32.88 2.12.88s1.55-.31 2.12-.88l7.17-7.17a3.009 3.009 0 00.01-4.24zm-1.42 2.82l-7.17 7.17c-.39.39-1.02.39-1.42 0L3 11.59V3h8.59l8.29 8.29c.39.39.39 1.03 0 1.41z" /><path d="M7.01 6C6.45 6 6 6.45 6 7s.45 1 1 1 1-.45 1-1-.44-1-.99-1z" /></g></svg>
        </a>
        <span class="mr-2"><a href="/tags/python/" class="no-link inline-flex items-center justify-center font-medium leading-none whitespace-nowrap text-blue-500 dark:text-blue-400 border border-blue-500 dark:border-blue-400 hover:bg-blue-100 dark:hover:bg-transparent dark:hover:border-blue-200 dark:hover:text-blue-200 transition-colors duration-200 ease-out h-6 px-2 text-xs rounded-md">
    <span>python</span>
</a></span>
        <span class="mr-2"><a href="/tags/machine-learning/" class="no-link inline-flex items-center justify-center font-medium leading-none whitespace-nowrap text-blue-500 dark:text-blue-400 border border-blue-500 dark:border-blue-400 hover:bg-blue-100 dark:hover:bg-transparent dark:hover:border-blue-200 dark:hover:text-blue-200 transition-colors duration-200 ease-out h-6 px-2 text-xs rounded-md">
    <span>machine learning</span>
</a></span>
        <span class="mr-2"><a href="/tags/nlp/" class="no-link inline-flex items-center justify-center font-medium leading-none whitespace-nowrap text-blue-500 dark:text-blue-400 border border-blue-500 dark:border-blue-400 hover:bg-blue-100 dark:hover:bg-transparent dark:hover:border-blue-200 dark:hover:text-blue-200 transition-colors duration-200 ease-out h-6 px-2 text-xs rounded-md">
    <span>nlp</span>
</a></span>
        <span class="mr-2"><a href="/tags/deep-learning/" class="no-link inline-flex items-center justify-center font-medium leading-none whitespace-nowrap text-blue-500 dark:text-blue-400 border border-blue-500 dark:border-blue-400 hover:bg-blue-100 dark:hover:bg-transparent dark:hover:border-blue-200 dark:hover:text-blue-200 transition-colors duration-200 ease-out h-6 px-2 text-xs rounded-md">
    <span>deep learning</span>
</a></span>
        <span class="mr-2"><a href="/tags/bert/" class="no-link inline-flex items-center justify-center font-medium leading-none whitespace-nowrap text-blue-500 dark:text-blue-400 border border-blue-500 dark:border-blue-400 hover:bg-blue-100 dark:hover:bg-transparent dark:hover:border-blue-200 dark:hover:text-blue-200 transition-colors duration-200 ease-out h-6 px-2 text-xs rounded-md">
    <span>bert</span>
</a></span>
    </span>
</div>


                <!-- Required only on API pages -->
                <doc-toolbar-member-filter-no-results />
            </div>

            
<nav class="flex mt-14">
    <div class="w-1/2">
        <a class="px-5 py-4 h-full flex items-center break-all md:break-normal font-medium text-blue-500 dark:text-blue-400 border border-gray-300 hover:border-gray-400 dark:border-dark-650 dark:hover:border-dark-450 rounded-l-lg transition-colors duration-150 relative hover:z-5" href="/2020/08/17/pyenv-jupyter.html/">
            <svg xmlns="http://www.w3.org/2000/svg" class="mr-3" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" overflow="visible"><path d="M19 11H7.41l5.29-5.29a.996.996 0 10-1.41-1.41l-7 7a1 1 0 000 1.42l7 7a1.024 1.024 0 001.42-.01.996.996 0 000-1.41L7.41 13H19c.55 0 1-.45 1-1s-.45-1-1-1z" /><path fill="none" d="M0 0h24v24H0z" /></svg>
            <span>
                <span class="block text-xs font-normal text-gray-400 dark:text-dark-400">Previous</span>
                <span class="block mt-1">pyenv, virtualenv and using them with Jupyter</span>
            </span>
        </a>
    </div>

    <div class="w-1/2">
        <a class="px-5 py-4 -mx-px h-full flex items-center justify-end break-all md:break-normal font-medium text-blue-500 dark:text-blue-400 border border-gray-300 hover:border-gray-400 dark:border-dark-650 dark:hover:border-dark-450 rounded-r-lg transition-colors duration-150 relative hover:z-5" href="/2020/06/15/python-trie.html/">
            <span>
                <span class="block text-xs font-normal text-right text-gray-400 dark:text-dark-400">Next</span>
                <span class="block mt-1">Implementing Trie in Python</span>
            </span>
            <svg xmlns="http://www.w3.org/2000/svg" class="ml-3" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" overflow="visible"><path d="M19.92 12.38a1 1 0 00-.22-1.09l-7-7a.996.996 0 10-1.41 1.41l5.3 5.3H5c-.55 0-1 .45-1 1s.45 1 1 1h11.59l-5.29 5.29a.996.996 0 000 1.41c.19.2.44.3.7.3s.51-.1.71-.29l7-7c.09-.09.16-.21.21-.33z" /><path fill="none" d="M0 0h24v24H0z" /></svg>
        </a>
    </div>
</nav>


        </main>

        <div class="border-t dark:border-dark-650 pt-6 mb-8">
            <footer class="flex flex-wrap items-center justify-between">
    <div>
        <ul class="flex flex-wrap items-center text-sm">
</ul>

    </div>
    <div class="docs-copyright py-2 text-gray-500 dark:text-dark-350 text-sm leading-relaxed"><p>© Copyright 2021. All rights reserved.</p>
</div>
</footer>

        </div>
    </div>
    
    <!-- Rendered if sidebar right is enabled -->
    <!-- Sidebar right skeleton-->
    <div v-cloak class="fixed top-0 bottom-0 right-0 transform translate-x-full bg-white border-gray-200 lg:sticky lg:border-l lg:flex-shrink-0 lg:pt-6 lg:transform-none lg:w-56 lg:z-0 md:w-72 sidebar-right skeleton dark:bg-dark-850 dark:border-dark-650">
        <div class="pl-5">
            <div class="w-32 h-3 mb-4 bg-gray-200 dark:bg-dark-600 rounded-full loading"></div>
            <div class="w-48 h-3 mb-4 bg-gray-200 dark:bg-dark-600 rounded-full loading"></div>
            <div class="w-40 h-3 mb-4 bg-gray-200 dark:bg-dark-600 rounded-full loading"></div>
        </div>
    </div>
    
    <!-- User should be able to hide sidebar right -->
    <doc-sidebar-right v-cloak></doc-sidebar-right>
</div>

        </div>
    </div>

    <doc-search-mobile></doc-search-mobile>
    <doc-back-to-top></doc-back-to-top>
</div>


        <div id="docs-overlay-target"></div>

        <script>window.__DOCS__ = { "title": "BERT - Tokenization and Encoding", icon: "file", hasPrism: true, hasMermaid: false, hasMath: false }</script>
    </body>
</html>
